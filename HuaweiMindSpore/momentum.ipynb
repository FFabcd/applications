{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18df4618-b8a6-4880-bfd7-f2be5351763d",
   "metadata": {},
   "source": [
    " # 写在前面\n",
    "<font color='red'> **我们使用了VGG19、AlexNet和Letnet3个网络来研究Momentum优化器不同参数的选择对模型训练带来的影响，并将结果转化为可视图像**</font>\n",
    " # 基于MindSpore框架的Momentum优化器案例实现\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346901c-f8b1-4e99-89b0-6fc7b67d2665",
   "metadata": {},
   "source": [
    "## 1、优化器基本原理讲解\n",
    "### ① 模型结构\n",
    "momentum算法 是一种用于计算损失函数最小值的算法。它属于常规梯度下降算法的一个改进算法。在原理上通过模拟物理中的动量概念，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少振荡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f1332-a376-4d4e-a749-c4960930b06c",
   "metadata": {},
   "source": [
    "### ② 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aced57-bcc2-4716-96a9-ddc72205fa92",
   "metadata": {},
   "source": [
    "- CIFAR10:  10类图\n",
    "- 训练集： 50k张图片\n",
    "- 测试集： 10k张图片\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc344fc5-2afe-40bd-ae17-22327dadd2f0",
   "metadata": {},
   "source": [
    "### ③ 相关背景\n",
    "#### 梯度（gradient）：在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。\n",
    "#### 梯度下降（Gradient Descent）：一个直观解释是比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b0e71-8f82-41d4-9a34-b8fd3ad1ae47",
   "metadata": {},
   "source": [
    "#### 动量优化器（momentum）：在使用梯度下降法时，每次都会朝着目标函数下降最快的方向，这也称为最速下降法。这种更新方法看似非常快，实际上存在一些问题。比如考虑一个二维输入，输出的损失函数L : R^2 → R 。这样在竖直方向上，梯度就非常大，在水平方向上，梯度就相对较小，所以我们在设置学习率的时候就不能设置太大，为了防止竖直方向上参数更新太过了，这样一个较小的学习率又导致了水平方向上参数在更新的时候太过于缓慢，所以就导致最终收敛起来非常慢。动量优化法是用之前 积累动量 来替代真正的梯度，每次迭代的梯度可以看作是运动的加速度。 \n",
    "#### 核心公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d9035-8034-414c-9de9-2b26db90388e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f172dbee-b7c9-4231-afec-a4e11fc2fc9a",
   "metadata": {},
   "source": [
    "#### 其中vi是当前速度，γ是动量参数，η是一个小于1的正数，η 是学习率。相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一直沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced14bf9-c708-4aee-88dc-f6205d636527",
   "metadata": {},
   "source": [
    "## 2、基于MindSpore框架的Momentum优化器自验结果结果及其分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a538b6-d7a6-4d6d-b2de-d826c66f90f7",
   "metadata": {},
   "source": [
    "# 1.VGG19\n",
    "## 应用案例介绍\n",
    "mindspore版本 1.7\n",
    "### 案例流程\n",
    "本案例使用数据集CIFAR10训练VGG网络模型，中间调用MindSpore提供的Momentum优化器API，并设置五组实验来对比Momentum优化器不同参数的选择对模型训练带来的影响。五组实验学习率固定设为0.01。实验一为Momentum超参数0.85，实验二为Momentum超参数0.8，实验三为Momentum超参数0.7，实验四为Momentum超参数0.6，实验五为SGD优化器。SGD，Momentum在理论上都能加速网络训练。最后通过实验数据可视化分析，得出结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaf25b-9bcd-43ed-9973-1e4fc641143c",
   "metadata": {},
   "source": [
    "### 关于数据集\n",
    "CIFAR-10 是由 Hinton 的学生 Alex Krizhevsky 和 Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含 10 个类别的 RGB 彩色图 片：飞机（ plane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。图片的尺寸为 32×32 ，数据集中一共有 50000 张训练圄片和 10000 张测试图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e7a3f-326d-402e-92d7-62ea6ab293c0",
   "metadata": {},
   "source": [
    "3.1.3 关于网络模型\n",
    "VGG是2014年Oxford的Visual Geometry Group提出的，其在在2014年的 ImageNet 大规模视觉识别挑(ILSVRC -2014中获得了亚军，第一名是GoogleNet。该网络是作者参加ILSVRC 2014比赛上的作者所做的相关工作，相比AlexNet，VGG使用了更深的网络结构，证明了增加网络深度能够在一定程度上影响网络性能。卷积网络的输入是224 * 224的RGB图像，整个网络的组成是非常格式化的，基本上都用的是3 * 3的卷积核以及 2 * 2的max pooling。网络结构如下：\n",
    "<img src=\"./vgg16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0ec06-5183-4907-9cd3-a0593be1ee28",
   "metadata": {},
   "source": [
    "3.2 实验步骤\n",
    "首先配置环境，要求MindSpore=1.7,还需要安装mindvision和解决兼容性问题。如果使用Jupyter Notebook做本实验，完成后需要重启内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3a2dfd-c73d-482a-b468-c49b5caeba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "from mindspore import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88aa29-5262-4ec5-abeb-ba984e370fea",
   "metadata": {},
   "source": [
    "由于mindvision支持几种经典数据集，其中就有Cifar10。我们直接使用mindvision接口下载Cifar10数据集，并且无需进行数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e820f978-4d59-4f7c-aeb8-11142426d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.dataset import Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ddcd9-9df7-41fc-8cfb-4890d6d45d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载并处理Cifar100数据集\n",
    "download_train = Cifar10(path=\"./cifar10\", split=\"train\", batch_size=32, repeat_num=1, shuffle=True, \n",
    "                         resize=32, download=True)\n",
    "\n",
    "download_eval = Cifar10(path=\"./cifar10\", split=\"test\", batch_size=32, resize=32, download=True)\n",
    "\n",
    "dataset_train = download_train.run()\n",
    "dataset_eval = download_eval.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f401e3d-65d3-4e78-9aff-1747e146dd99",
   "metadata": {},
   "source": [
    "检查数据集结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963ef629-df67-49f5-a414-d253a30b082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (32, 3, 32, 32) Float32\n",
      "Shape of label: (32,) Int32\n"
     ]
    }
   ],
   "source": [
    "# 看一下cifar10的格式\n",
    "for image, label in dataset_train.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252e4b5-2352-4e05-887b-3e005a418d2f",
   "metadata": {},
   "source": [
    "VGG16网络原定输入是224*224，但是Cifar10仅为32*32，所以不能直接使用mindspore提供的网络，需要自行定义。同时在原网络的每一个卷积层之后添加了批归一化层，加快收敛速度。在全连接层添加了dropout层以抑制过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2db5c26-746d-42ad-93c1-08e178d2bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "\n",
    "import mindspore.nn as nn\n",
    "\n",
    "class VGG16(nn.Cell):\n",
    "    \"\"\"\n",
    "    VGG16网络结构\n",
    "    \"\"\"\n",
    "    def __init__(self, num_class=10, num_channel=3):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        # 卷积层，输入的通道数为num_channel，输出的通道数为64，卷积核大小为3*3\n",
    "        self.conv1 = nn.Conv2d(num_channel, 64, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        # 卷积层，输入的通道数为64，输出的通道数为64，卷积核大小为3*3\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        \n",
    "        # 卷积层，输入的通道数为64，输出的通道数为128，卷积核大小为3*3\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        # 卷积层，输入的通道数为128，输出的通道数为128，卷积核大小为3*3\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=128)\n",
    "        # 池化层\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        # 卷积层，输入的通道数为128，输出的通道数为256，卷积核大小为3*3\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        # 卷积层，输入的通道数为256，输出的通道数为256，卷积核大小为3*3\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=256)\n",
    "        # 卷积层，输入的通道数为256，输出的通道数为256，卷积核大小为3*3\n",
    "        self.conv7 = nn.Conv2d(256, 256, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn7 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        \n",
    "         # 卷积层，输入的通道数为256，输出的通道数为512，卷积核大小为3*3\n",
    "        self.conv8 = nn.Conv2d(256, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn8 = nn.BatchNorm2d(num_features=512)\n",
    "        # 卷积层，输入的通道数为512，输出的通道数为512，卷积核大小为3*3\n",
    "        self.conv9 = nn.Conv2d(512, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn9 = nn.BatchNorm2d(num_features=512)\n",
    "        # 卷积层，输入的通道数为256，输出的通道数为256，卷积核大小为3*3\n",
    "        self.conv10 = nn.Conv2d(512, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn10 = nn.BatchNorm2d(num_features=512)\n",
    "        # 池化层\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 卷积层，输入的通道数为512，输出的通道数为512，卷积核大小为3*3\n",
    "        self.conv11 = nn.Conv2d(512, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn11 = nn.BatchNorm2d(num_features=512)\n",
    "        # 卷积层，输入的通道数为512，输出的通道数为512，卷积核大小为3*3\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn12 = nn.BatchNorm2d(num_features=512)\n",
    "        # 卷积层，输入的通道数为256，输出的通道数为256，卷积核大小为3*3\n",
    "        self.conv13 = nn.Conv2d(512, 512, 3, pad_mode='same')\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 批归一化层\n",
    "        self.bn13 = nn.BatchNorm2d(num_features=512)\n",
    "        \n",
    "        # 多维数组展平为一维数组\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 全连接层，输入个数为8*8*512，输出个数为4096\n",
    "        self.fc1 = nn.Dense(8*8*512, 4096)\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 全连接层，输入个数为4096，输出个数为4096\n",
    "        self.fc2 = nn.Dense(4096, 4096)\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 全连接层，输入个数为4096，输出个数为1000\n",
    "        self.fc3 = nn.Dense(4096, 1000)\n",
    "        # dropout正则化，抑制过拟合\n",
    "        self.dropout = nn.Dropout(keep_prob=0.9)\n",
    "        # 全连接层，输入个数为1000，输出个数为10\n",
    "        self.fc4 = nn.Dense(1000, 10)\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 使用定义好的运算构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn10(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv11(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn13(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "network = VGG16(num_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6252ae29-6d53-4863-808b-beec69e2acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# 定义优化器函数\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc490d18-68f0-4e3b-9f6f-03cd8cbc9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "\n",
    "# 设置模型保存参数，模型训练保存参数的step为1562\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1562, keep_checkpoint_max=20)\n",
    "\n",
    "# 应用模型保存参数\n",
    "ckpoint = ModelCheckpoint(prefix=\"vgg16\", directory=\"./vgg16\", config=config_ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e767c9-32b0-4d73-aca2-bb0c39c522e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[  0/ 20], step:[ 1562/ 1562], loss:[1.008/1.008], time:115005.525 ms, lr:0.01000\n",
      "Epoch time: 121402.980 ms, per step time: 77.723 ms, avg loss: 1.008\n",
      "Epoch:[  1/ 20], step:[ 1562/ 1562], loss:[0.947/0.947], time:93004.545 ms, lr:0.01000\n",
      "Epoch time: 93013.749 ms, per step time: 59.548 ms, avg loss: 0.947\n",
      "Epoch:[  2/ 20], step:[ 1562/ 1562], loss:[0.645/0.645], time:93022.658 ms, lr:0.01000\n",
      "Epoch time: 93028.489 ms, per step time: 59.557 ms, avg loss: 0.645\n",
      "Epoch:[  3/ 20], step:[ 1562/ 1562], loss:[0.849/0.849], time:92921.094 ms, lr:0.01000\n",
      "Epoch time: 92932.289 ms, per step time: 59.496 ms, avg loss: 0.849\n",
      "Epoch:[  4/ 20], step:[ 1562/ 1562], loss:[0.625/0.625], time:96025.863 ms, lr:0.01000\n",
      "Epoch time: 96033.432 ms, per step time: 61.481 ms, avg loss: 0.625\n",
      "Epoch:[  5/ 20], step:[ 1562/ 1562], loss:[0.368/0.368], time:94054.955 ms, lr:0.01000\n",
      "Epoch time: 94060.778 ms, per step time: 60.218 ms, avg loss: 0.368\n",
      "Epoch:[  6/ 20], step:[ 1562/ 1562], loss:[0.223/0.223], time:92969.829 ms, lr:0.01000\n",
      "Epoch time: 92981.252 ms, per step time: 59.527 ms, avg loss: 0.223\n",
      "Epoch:[  7/ 20], step:[ 1562/ 1562], loss:[0.444/0.444], time:95971.643 ms, lr:0.01000\n",
      "Epoch time: 95976.737 ms, per step time: 61.445 ms, avg loss: 0.444\n",
      "Epoch:[  8/ 20], step:[ 1562/ 1562], loss:[0.540/0.540], time:94014.103 ms, lr:0.01000\n",
      "Epoch time: 94018.456 ms, per step time: 60.191 ms, avg loss: 0.540\n",
      "Epoch:[  9/ 20], step:[ 1562/ 1562], loss:[0.372/0.372], time:95905.105 ms, lr:0.01000\n",
      "Epoch time: 95909.528 ms, per step time: 61.402 ms, avg loss: 0.372\n",
      "Epoch:[ 10/ 20], step:[ 1562/ 1562], loss:[0.205/0.205], time:92036.258 ms, lr:0.01000\n",
      "Epoch time: 92040.243 ms, per step time: 58.925 ms, avg loss: 0.205\n",
      "Epoch:[ 11/ 20], step:[ 1562/ 1562], loss:[0.253/0.253], time:94974.339 ms, lr:0.01000\n",
      "Epoch time: 94978.494 ms, per step time: 60.806 ms, avg loss: 0.253\n",
      "Epoch:[ 12/ 20], step:[ 1562/ 1562], loss:[0.171/0.171], time:93070.340 ms, lr:0.01000\n",
      "Epoch time: 93074.363 ms, per step time: 59.587 ms, avg loss: 0.171\n",
      "Epoch:[ 13/ 20], step:[ 1562/ 1562], loss:[0.143/0.143], time:93025.936 ms, lr:0.01000\n",
      "Epoch time: 93030.062 ms, per step time: 59.558 ms, avg loss: 0.143\n",
      "Epoch:[ 14/ 20], step:[ 1562/ 1562], loss:[0.280/0.280], time:93959.079 ms, lr:0.01000\n",
      "Epoch time: 93963.211 ms, per step time: 60.156 ms, avg loss: 0.280\n",
      "Epoch:[ 15/ 20], step:[ 1562/ 1562], loss:[0.060/0.060], time:91979.751 ms, lr:0.01000\n",
      "Epoch time: 91989.715 ms, per step time: 58.892 ms, avg loss: 0.060\n",
      "Epoch:[ 16/ 20], step:[ 1562/ 1562], loss:[0.033/0.033], time:94996.954 ms, lr:0.01000\n",
      "Epoch time: 95001.676 ms, per step time: 60.821 ms, avg loss: 0.033\n",
      "Epoch:[ 17/ 20], step:[ 1562/ 1562], loss:[0.251/0.251], time:94965.876 ms, lr:0.01000\n",
      "Epoch time: 94970.009 ms, per step time: 60.800 ms, avg loss: 0.251\n",
      "Epoch:[ 18/ 20], step:[ 1562/ 1562], loss:[0.320/0.320], time:93945.869 ms, lr:0.01000\n",
      "Epoch time: 93950.310 ms, per step time: 60.147 ms, avg loss: 0.320\n",
      "Epoch:[ 19/ 20], step:[ 1562/ 1562], loss:[0.139/0.139], time:93077.806 ms, lr:0.01000\n",
      "Epoch time: 93082.281 ms, per step time: 59.592 ms, avg loss: 0.139\n"
     ]
    }
   ],
   "source": [
    "from mindvision.engine.callback import LossMonitor\n",
    "from mindspore.train import Model\n",
    "\n",
    "# 初始化模型参数\n",
    "model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={'accuracy'})\n",
    "\n",
    "# 训练网络模型，并保存为vgg16-1_1562.ckpt文件\n",
    "model.train(20, dataset_train, callbacks=[ckpoint, LossMonitor(0.01, 1562)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c1c5f9-5b73-4607-8222-df405a00c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8962339743589743}\n"
     ]
    }
   ],
   "source": [
    "acc = model.eval(dataset_eval)\n",
    "\n",
    "print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5083abea-4cf3-439b-934b-3f3ed399d30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom mindspore import load_checkpoint, load_param_into_net\\n# 看一下每轮的acc\\nfor i in range(1,21):\\n    path = \"./vgg16-2/vgg16-2-\" + str(i) + \"_1562.ckpt\"\\n    # 加载已经保存的用于测试的模型\\n    param_dict = load_checkpoint(path)\\n    # 加载参数到网络中\\n    load_param_into_net(network, param_dict)\\n    # 初始化模型参数\\n    model = Model(network,loss_fn=net_loss, optimizer=net_opt,metrics={\\'accuracy\\'})\\n    dataset_eval = download_eval.run()\\n    acc = model.eval(dataset_eval)\\n    print(\"epoch{},{}\".format(i, acc))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "# 看一下每轮的acc\n",
    "for i in range(1,21):\n",
    "    path = \"./vgg16-2/vgg16-2-\" + str(i) + \"_1562.ckpt\"\n",
    "    # 加载已经保存的用于测试的模型\n",
    "    param_dict = load_checkpoint(path)\n",
    "    # 加载参数到网络中\n",
    "    load_param_into_net(network, param_dict)\n",
    "    # 初始化模型参数\n",
    "    model = Model(network,loss_fn=net_loss, optimizer=net_opt,metrics={'accuracy'})\n",
    "    dataset_eval = download_eval.run()\n",
    "    acc = model.eval(dataset_eval)\n",
    "    print(\"epoch{},{}\".format(i, acc))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63ec89-1933-46b9-b821-1bcd54284d2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de53ef3a-1d77-499b-abdf-bc69e1ecc2f9",
   "metadata": {},
   "source": [
    "# 2.AlexNet\n",
    "使用AlexnetNet对Momentum不同的值进行对比，探究Momentum取值对于loss值以及对Accuracy的影响，Momentum修在在ymal文件中 mindspore版本 1.8\n",
    "## alexnet介绍\n",
    "在2010年的ImageNet LSVRC-2010上，AlexNet在给包含有1000种类别的共120万张高分辨率图片的分类任务中，在测试集上的top-1和top-5错误率为37.5%和17.0%（top-5 错误率：即对一张图像预测5个类别，只要有一个和人工标注类别相同就算对，否则算错。同理top-1对一张图像只预测1个类别），在ImageNet LSVRC-2012的比赛中，取得了top-5错误率为15.3%的成绩。AlexNet有6亿个参数和650,000个神经元，包含5个卷积层，有些层后面跟了max-pooling层，3个全连接层，为了减少过拟合，在全连接层使用了dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8de376-d230-40ea-a166-387dc7df5288",
   "metadata": {},
   "source": [
    "## 获取配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54fdad8-e986-49c3-8983-f6bd747e59b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enable_modelarts': 'Whether training on modelarts, default: False', 'data_url': 'Dataset url for obs', 'train_url': 'Training output url for obs', 'data_path': 'Dataset path for local', 'output_path': 'Training output path for local', 'device_target': 'Target device type', 'enable_profiling': 'Whether enable profiling while training, default: False'}\n",
      "{'air_name': 'alexnet.air',\n",
      " 'batch_size': 32,\n",
      " 'buffer_size': 1000,\n",
      " 'checkpoint_file': './checkpoint/checkpoint_alexnet-30_1562.ckpt',\n",
      " 'checkpoint_path': 'train/',\n",
      " 'checkpoint_url': '',\n",
      " 'ckpt_file': '/cache/train/checkpoint_alexnet-30_1562.ckpt',\n",
      " 'ckpt_path': '/cache/train',\n",
      " 'config_path': '/home/ma-user/work/VGG/default_config.yaml',\n",
      " 'data_path': '/home/ma-user/work/VGG/cifar-10-batches-bin',\n",
      " 'data_url': '',\n",
      " 'dataset_name': 'cifar10',\n",
      " 'dataset_sink_mode': True,\n",
      " 'device_id': 0,\n",
      " 'device_target': 'Ascend',\n",
      " 'enable_modelarts': False,\n",
      " 'enable_profiling': False,\n",
      " 'epoch_size': 30,\n",
      " 'file_format': 'MINDIR',\n",
      " 'file_name': 'alexnet',\n",
      " 'image_height': 227,\n",
      " 'image_width': 227,\n",
      " 'keep_checkpoint_max': 10,\n",
      " 'learning_rate': 0.002,\n",
      " 'load_path': '/cache/checkpoint_path',\n",
      " 'lr': 0.01,\n",
      " 'model_name': 'alexnet',\n",
      " 'momentum': 0.9,\n",
      " 'num_classes': 10,\n",
      " 'num_parallel_workers': 8,\n",
      " 'output_path': '/cache/train',\n",
      " 'save_checkpoint': True,\n",
      " 'save_checkpoint_epochs': 2,\n",
      " 'save_checkpoint_steps': 1562,\n",
      " 'sink_size': -1,\n",
      " 'train_url': ''}\n"
     ]
    }
   ],
   "source": [
    "from mindspore import context\n",
    "import os\n",
    "import ast\n",
    "import argparse\n",
    "from pprint import pprint, pformat\n",
    "import yaml\n",
    "from mindspore.communication.management import init, get_rank\n",
    "from mindspore.context import ParallelMode\n",
    "import sys\n",
    "from mindspore import context\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration namespace. Convert dictionary to members.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg_dict):\n",
    "        for k, v in cfg_dict.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                setattr(self, k, [Config(x) if isinstance(x, dict) else x for x in v])\n",
    "            else:\n",
    "                setattr(self, k, Config(v) if isinstance(v, dict) else v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pformat(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "def parse_cli_to_yaml(parser, cfg, helper=None, choices=None, cfg_path=\"default_config.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse command line arguments to the configuration according to the default yaml.\n",
    "\n",
    "    Args:\n",
    "        parser: Parent parser.\n",
    "        cfg: Base configuration.\n",
    "        helper: Helper description.\n",
    "        cfg_path: Path to the default yaml config.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"[REPLACE THIS at config.py]\",\n",
    "                                     parents=[parser])\n",
    "    helper = {} if helper is None else helper\n",
    "    choices = {} if choices is None else choices\n",
    "    for item in cfg:\n",
    "        if not isinstance(cfg[item], list) and not isinstance(cfg[item], dict):\n",
    "            help_description = helper[item] if item in helper else \"Please reference to {}\".format(cfg_path)\n",
    "            choice = choices[item] if item in choices else None\n",
    "            if isinstance(cfg[item], bool):\n",
    "                parser.add_argument(\"--\" + item, type=ast.literal_eval, default=cfg[item], choices=choice,\n",
    "                                    help=help_description)\n",
    "            else:\n",
    "                parser.add_argument(\"--\" + item, type=type(cfg[item]), default=cfg[item], choices=choice,\n",
    "                                    help=help_description)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def parse_yaml(yaml_path):\n",
    "    \"\"\"\n",
    "    Parse the yaml config file.\n",
    "\n",
    "    Args:\n",
    "        yaml_path: Path to the yaml config.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as fin:\n",
    "        try:\n",
    "            cfgs = yaml.load_all(fin.read(), Loader=yaml.FullLoader)\n",
    "            cfgs = [x for x in cfgs]\n",
    "            if len(cfgs) == 1:\n",
    "                cfg_helper = {}\n",
    "                cfg = cfgs[0]\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 2:\n",
    "                cfg, cfg_helper = cfgs\n",
    "                cfg_choices = {}\n",
    "            elif len(cfgs) == 3:\n",
    "                cfg, cfg_helper, cfg_choices = cfgs\n",
    "            else:\n",
    "                raise ValueError(\"At most 3 docs (config, description for help, choices) are supported in config yaml\")\n",
    "            print(cfg_helper)\n",
    "        except:\n",
    "            raise ValueError(\"Failed to parse yaml\")\n",
    "    return cfg, cfg_helper, cfg_choices\n",
    "\n",
    "\n",
    "def merge(args, cfg):\n",
    "    \"\"\"\n",
    "    Merge the base config from yaml file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        args: Command line arguments.\n",
    "        cfg: Base configuration.\n",
    "    \"\"\"\n",
    "    args_var = vars(args)\n",
    "    for item in args_var:\n",
    "        cfg[item] = args_var[item]\n",
    "    return cfg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sys.argv =[' python train.py --config_path default_config.yaml --device_target \"Ascend\" --data_path cifar-10-batches-bin --ckpt_path ckpt']\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"default name\", add_help=False)\n",
    "current_dir = os.getcwd()\n",
    "parser.add_argument(\"--config_path\", type=str, default=os.path.join(current_dir, \"default_config.yaml\"),\n",
    "                        help=\"Config file path\")\n",
    "path_args, _ = parser.parse_known_args()\n",
    "default, helper, choices = parse_yaml(path_args.config_path)\n",
    "args= parse_cli_to_yaml(parser=parser, cfg=default, helper=helper, choices=choices, cfg_path=path_args.config_path)\n",
    "final_config = merge(args, default)\n",
    "pprint(final_config)\n",
    "config=Config(final_config)\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")\n",
    "context.set_context(save_graphs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b6e29-071c-4dc1-a15f-d7574465f6e7",
   "metadata": {},
   "source": [
    "## 获取device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a17d4d3-594c-4038-a48e-5d06ab46fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Moxing adapter for ModelArts\"\"\"\n",
    "\n",
    "import os\n",
    "import functools\n",
    "from mindspore import context\n",
    "from mindspore.profiler import Profiler\n",
    "\n",
    "\n",
    "_global_sync_count = 0\n",
    "\n",
    "def get_device_id():\n",
    "    device_id = os.getenv('DEVICE_ID', '0')\n",
    "    return int(device_id)\n",
    "\n",
    "\n",
    "def get_device_num():\n",
    "    device_num = os.getenv('RANK_SIZE', '1')\n",
    "    return int(device_num)\n",
    "\n",
    "\n",
    "def get_rank_id():\n",
    "    global_rank_id = os.getenv('RANK_ID', '0')\n",
    "    return int(global_rank_id)\n",
    "\n",
    "\n",
    "def get_job_id():\n",
    "    job_id = os.getenv('JOB_ID')\n",
    "    job_id = job_id if job_id != \"\" else \"default\"\n",
    "    return job_id\n",
    "\n",
    "def sync_data(from_path, to_path):\n",
    "    \"\"\"\n",
    "    Download data from remote obs to local directory if the first url is remote url and the second one is local path\n",
    "    Upload data from local directory to remote obs in contrast.\n",
    "    \"\"\"\n",
    "    import moxing as mox\n",
    "    import time\n",
    "    global _global_sync_count\n",
    "    sync_lock = \"/tmp/copy_sync.lock\" + str(_global_sync_count)\n",
    "    _global_sync_count += 1\n",
    "\n",
    "    # Each server contains 8 devices as most.\n",
    "    if get_device_id() % min(get_device_num(), 8) == 0 and not os.path.exists(sync_lock):\n",
    "        print(\"from path: \", from_path)\n",
    "        print(\"to path: \", to_path)\n",
    "        mox.file.copy_parallel(from_path, to_path)\n",
    "        print(\"===finish data synchronization===\")\n",
    "        try:\n",
    "            os.mknod(sync_lock)\n",
    "        except IOError:\n",
    "            pass\n",
    "        print(\"===save flag===\")\n",
    "\n",
    "    while True:\n",
    "        if os.path.exists(sync_lock):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Finish sync data from {} to {}.\".format(from_path, to_path))\n",
    "\n",
    "\n",
    "def moxing_wrapper(pre_process=None, post_process=None):\n",
    "    \"\"\"\n",
    "    Moxing wrapper to download dataset and upload outputs.\n",
    "    \"\"\"\n",
    "    def wrapper(run_func):\n",
    "        @functools.wraps(run_func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            # Download data from data_url\n",
    "            if config.enable_modelarts:\n",
    "                if config.data_url:\n",
    "                    sync_data(config.data_url, config.data_path)\n",
    "                    print(\"Dataset downloaded: \", os.listdir(config.data_path))\n",
    "                if config.checkpoint_url:\n",
    "                    sync_data(config.checkpoint_url, config.load_path)\n",
    "                    print(\"Preload downloaded: \", os.listdir(config.load_path))\n",
    "                if config.train_url:\n",
    "                    sync_data(config.train_url, config.output_path)\n",
    "                    print(\"Workspace downloaded: \", os.listdir(config.output_path))\n",
    "\n",
    "                context.set_context(save_graphs_path=os.path.join(config.output_path, str(get_rank_id())))\n",
    "                config.device_num = get_device_num()\n",
    "                config.device_id = get_device_id()\n",
    "                if not os.path.exists(config.output_path):\n",
    "                    os.makedirs(config.output_path)\n",
    "\n",
    "                if pre_process:\n",
    "                    pre_process()\n",
    "\n",
    "            if config.enable_profiling:\n",
    "                profiler = Profiler()\n",
    "\n",
    "            run_func(*args, **kwargs)\n",
    "\n",
    "            if config.enable_profiling:\n",
    "                profiler.analyse()\n",
    "\n",
    "            # Upload data to train_url\n",
    "            if config.enable_modelarts:\n",
    "                if post_process:\n",
    "                    post_process()\n",
    "\n",
    "                if config.train_url:\n",
    "                    print(\"Start to copy output directory\")\n",
    "                    sync_data(config.output_path, config.train_url)\n",
    "        return wrapped_func\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d7ccc8-8da9-4425-baa0-63854590b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = get_device_num()\n",
    "\n",
    "if device_num > 1:\n",
    "        context.reset_auto_parallel_context()\n",
    "        context.set_auto_parallel_context(device_num=device_num, \\\n",
    "            parallel_mode=ParallelMode.DATA_PARALLEL, gradients_mean=True)\n",
    "        \n",
    "        context.set_context(device_id=get_device_id())\n",
    "        init()\n",
    "else:\n",
    "        context.set_context(device_id=get_device_id())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc5fa3-3a71-443d-9573-fd7687ba7423",
   "metadata": {},
   "source": [
    "## 获取秩大小和秩id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef3ef14-bd5d-46fc-9eaa-bb6f5fdf1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.communication.management import get_rank, get_group_size\n",
    "\n",
    "def _get_rank_info():\n",
    "    \"\"\"\n",
    "    get rank size and rank id\n",
    "    \"\"\"\n",
    "    rank_size = int(os.environ.get(\"RANK_SIZE\", 1))\n",
    "\n",
    "    if rank_size > 1:\n",
    "        rank_size = get_group_size()\n",
    "        rank_id = get_rank()\n",
    "    else:\n",
    "        rank_size = 1\n",
    "        rank_id = 0\n",
    "\n",
    "    return rank_size, rank_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b170edc-927c-47a6-bb06-6ba26a20754d",
   "metadata": {},
   "source": [
    "## 获取、处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff839c1-1d99-4717-adf2-5e97a46309cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/VGG/cifar-10-batches-bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as C\n",
    "import mindspore.dataset.vision as CV\n",
    "from mindspore.common import dtype as mstype\n",
    "from mindspore.communication.management import get_rank, get_group_size\n",
    "\n",
    "cfg=config\n",
    "data_path= config.data_path\n",
    "print(config.data_path)\n",
    "batch_size=config.batch_size\n",
    "status=\"train\"\n",
    "target=\"Ascend\"\n",
    "num_parallel_workers=8\n",
    "device_num, rank_id = _get_rank_info()\n",
    "ds.config.set_prefetch_size(64)\n",
    "if target == \"Ascend\":\n",
    "        device_num, rank_id = _get_rank_info()\n",
    "\n",
    "if target != \"Ascend\" or device_num == 1:\n",
    "        cifar_ds = ds.Cifar10Dataset(data_path, shuffle=True)\n",
    "else:\n",
    "        cifar_ds = ds.Cifar10Dataset(data_path, num_parallel_workers=num_parallel_workers,\n",
    "                                     shuffle=True, num_shards=device_num, shard_id=rank_id)\n",
    "rescale = 1.0 / 255.0\n",
    "shift = 0.0\n",
    "    # cfg = alexnet_cifar10_cfg\n",
    "\n",
    "resize_op = CV.Resize((cfg.image_height, cfg.image_width))\n",
    "rescale_op = CV.Rescale(rescale, shift)\n",
    "if status == \"train\":\n",
    "    random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4])\n",
    "    random_horizontal_op = CV.RandomHorizontalFlip()\n",
    "typecast_op = C.TypeCast(mstype.int32)\n",
    "cifar_ds = cifar_ds.map(input_columns=\"label\", operations=typecast_op,\n",
    "                            num_parallel_workers=1)\n",
    "if status == \"train\":\n",
    "        compose_op = [random_crop_op, random_horizontal_op, resize_op, rescale_op]\n",
    "else:\n",
    "        compose_op = [resize_op, rescale_op]\n",
    "cifar_ds = cifar_ds.map(input_columns=\"image\", operations=compose_op, num_parallel_workers=num_parallel_workers)\n",
    "\n",
    "ds_train = cifar_ds.batch(batch_size, drop_remainder=True)\n",
    " \n",
    "if ds_train.get_dataset_size() == 0:\n",
    "        raise ValueError(\"Please check dataset size > 0 and batch_size <= dataset size\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f83070-5923-4168-96e3-858955ff2dfe",
   "metadata": {},
   "source": [
    "## 构建alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e05afe-7896-4fce-8321-c053b92fde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common.tensor import Tensor\n",
    "import mindspore.common.dtype as mstype\n",
    "\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train import Model\n",
    "def get_lr_cifar10(current_step, lr_max, total_epochs, steps_per_epoch):\n",
    "    \"\"\"\n",
    "    generate learning rate array\n",
    "\n",
    "    Args:\n",
    "       current_step(int): current steps of the training\n",
    "       lr_max(float): max learning rate\n",
    "       total_epochs(int): total epoch of training\n",
    "       steps_per_epoch(int): steps of one epoch\n",
    "\n",
    "    Returns:\n",
    "       np.array, learning rate array\n",
    "    \"\"\"\n",
    "    lr_each_step = []\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    decay_epoch_index = [0.8 * total_steps]\n",
    "    for i in range(total_steps):\n",
    "        if i < decay_epoch_index[0]:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = lr_max * 0.1\n",
    "        lr_each_step.append(lr)\n",
    "    lr_each_step = np.array(lr_each_step).astype(np.float32)\n",
    "    learning_rate = lr_each_step[current_step:]\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0, pad_mode=\"valid\", has_bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                     has_bias=has_bias, pad_mode=pad_mode)\n",
    "\n",
    "def fc_with_initialize(input_channels, out_channels, has_bias=True):\n",
    "    return nn.Dense(input_channels, out_channels, has_bias=has_bias)\n",
    "\n",
    "class DataNormTranspose(nn.Cell):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respectively.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respectively.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name='imagenet'):\n",
    "        super(DataNormTranspose, self).__init__()\n",
    "        # Computed from random subset of ImageNet training images\n",
    "        if dataset_name == 'imagenet':\n",
    "            self.mean = Tensor(np.array([0.485 * 255, 0.456 * 255, 0.406 * 255]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "            self.std = Tensor(np.array([0.229 * 255, 0.224 * 255, 0.225 * 255]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "        else:\n",
    "            self.mean = Tensor(np.array([0.4914, 0.4822, 0.4465]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "            self.std = Tensor(np.array([0.2023, 0.1994, 0.2010]).reshape((1, 1, 1, 3)), mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = (x - self.mean) / self.std\n",
    "        x = F.transpose(x, (0, 3, 1, 2))\n",
    "        return x\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, channel=3, phase='train', include_top=True, dataset_name='imagenet'):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.data_trans = DataNormTranspose(dataset_name=dataset_name)\n",
    "        self.conv1 = conv(channel, 64, 11, stride=4, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv2 = conv(64, 128, 5, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv3 = conv(128, 192, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv4 = conv(192, 256, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.conv5 = conv(256, 256, 3, pad_mode=\"same\", has_bias=True)\n",
    "        self.relu = P.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='valid')\n",
    "        self.include_top = include_top\n",
    "        if self.include_top:\n",
    "            dropout_ratio = 0.65\n",
    "            if phase == 'test':\n",
    "                dropout_ratio = 1.0\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = fc_with_initialize(6 * 6 * 256, 4096)\n",
    "            self.fc2 = fc_with_initialize(4096, 4096)\n",
    "            self.fc3 = fc_with_initialize(4096, num_classes)\n",
    "            self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"define network\"\"\"\n",
    "        x = self.data_trans(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "network = AlexNet(config.num_classes, phase='train', dataset_name=config.dataset_name)\n",
    "loss_scale_manager = None\n",
    "metrics = None\n",
    "step_per_epoch = ds_train.get_dataset_size() if config.sink_size == -1 else config.sink_size\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "lr = Tensor(get_lr_cifar10(0, config.learning_rate, config.epoch_size, step_per_epoch))\n",
    "opt = nn.Momentum(network.trainable_params(), lr, config.momentum)\n",
    "metrics = {\"Accuracy\": Accuracy()}\n",
    "model = Model(network, loss_fn=loss, optimizer=opt, metrics=metrics, amp_level=\"O2\",\n",
    "                      loss_scale_manager=loss_scale_manager)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb7435-f84d-4a3b-9bd8-8a70c831fbe8",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3350bf4-2b84-4547-8a55-505f087950f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(25436,ffff8a6fc780,python):2022-11-07-22:20:15.595.999 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-TrainOneStepCell/network-WithLossCell/_backbone-AlexNet/dropout-Dropout/DropoutGenMask-op212] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(25436,ffff8a6fc780,python):2022-11-07-22:20:15.596.220 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-TrainOneStepCell/network-WithLossCell/_backbone-AlexNet/dropout-Dropout/DropoutGenMask-op213] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1875, loss is 1.8004567623138428\n",
      "Train epoch time: 40146.073 ms, per step time: 21.411 ms\n",
      "epoch: 2 step: 1875, loss is 1.380813479423523\n",
      "Train epoch time: 12475.951 ms, per step time: 6.654 ms\n",
      "epoch: 3 step: 1875, loss is 1.3110417127609253\n",
      "Train epoch time: 12160.276 ms, per step time: 6.485 ms\n",
      "epoch: 4 step: 1875, loss is 0.8924959301948547\n",
      "Train epoch time: 12403.024 ms, per step time: 6.615 ms\n",
      "epoch: 5 step: 1875, loss is 0.858462393283844\n",
      "Train epoch time: 12588.364 ms, per step time: 6.714 ms\n",
      "epoch: 6 step: 1875, loss is 0.7013843059539795\n",
      "Train epoch time: 12512.207 ms, per step time: 6.673 ms\n",
      "epoch: 7 step: 1875, loss is 0.8644324541091919\n",
      "Train epoch time: 12124.501 ms, per step time: 6.466 ms\n",
      "epoch: 8 step: 1875, loss is 0.8506056070327759\n",
      "Train epoch time: 12473.443 ms, per step time: 6.653 ms\n",
      "epoch: 9 step: 1875, loss is 0.45187246799468994\n",
      "Train epoch time: 12249.605 ms, per step time: 6.533 ms\n",
      "epoch: 10 step: 1875, loss is 0.4801371693611145\n",
      "Train epoch time: 12639.324 ms, per step time: 6.741 ms\n",
      "epoch: 11 step: 1875, loss is 0.6990430951118469\n",
      "Train epoch time: 12468.384 ms, per step time: 6.650 ms\n",
      "epoch: 12 step: 1875, loss is 0.33393099904060364\n",
      "Train epoch time: 12658.234 ms, per step time: 6.751 ms\n",
      "epoch: 13 step: 1875, loss is 0.29373809695243835\n",
      "Train epoch time: 12535.902 ms, per step time: 6.686 ms\n",
      "epoch: 14 step: 1875, loss is 0.4943583905696869\n",
      "Train epoch time: 12787.836 ms, per step time: 6.820 ms\n",
      "epoch: 15 step: 1875, loss is 0.47169601917266846\n",
      "Train epoch time: 12510.198 ms, per step time: 6.672 ms\n",
      "epoch: 16 step: 1875, loss is 0.29789140820503235\n",
      "Train epoch time: 12526.639 ms, per step time: 6.681 ms\n",
      "epoch: 17 step: 1875, loss is 0.34822553396224976\n",
      "Train epoch time: 12479.011 ms, per step time: 6.655 ms\n",
      "epoch: 18 step: 1875, loss is 0.3208097517490387\n",
      "Train epoch time: 12497.142 ms, per step time: 6.665 ms\n",
      "epoch: 19 step: 1875, loss is 0.3825042247772217\n",
      "Train epoch time: 12446.315 ms, per step time: 6.638 ms\n",
      "epoch: 20 step: 1875, loss is 0.412689745426178\n",
      "Train epoch time: 12509.594 ms, per step time: 6.672 ms\n",
      "epoch: 21 step: 1875, loss is 0.2140849381685257\n",
      "Train epoch time: 12536.111 ms, per step time: 6.686 ms\n",
      "epoch: 22 step: 1875, loss is 0.36184921860694885\n",
      "Train epoch time: 12225.084 ms, per step time: 6.520 ms\n",
      "epoch: 23 step: 1875, loss is 0.3288807272911072\n",
      "Train epoch time: 12303.856 ms, per step time: 6.562 ms\n",
      "epoch: 24 step: 1875, loss is 0.23251497745513916\n",
      "Train epoch time: 12633.196 ms, per step time: 6.738 ms\n",
      "epoch: 25 step: 1875, loss is 0.3067978620529175\n",
      "Train epoch time: 12550.137 ms, per step time: 6.693 ms\n",
      "epoch: 26 step: 1875, loss is 0.2567673921585083\n",
      "Train epoch time: 12450.114 ms, per step time: 6.640 ms\n",
      "epoch: 27 step: 1875, loss is 0.15852409601211548\n",
      "Train epoch time: 12585.511 ms, per step time: 6.712 ms\n",
      "epoch: 28 step: 1875, loss is 0.15912550687789917\n",
      "Train epoch time: 12511.909 ms, per step time: 6.673 ms\n",
      "epoch: 29 step: 1875, loss is 0.04158003628253937\n",
      "Train epoch time: 12692.963 ms, per step time: 6.770 ms\n",
      "epoch: 30 step: 1875, loss is 0.022528432309627533\n",
      "Train epoch time: 12513.968 ms, per step time: 6.674 ms\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "model = Model(network, loss_fn=loss, optimizer=opt, metrics=metrics, amp_level=\"O2\", keep_batchnorm_fp32=False,\n",
    "                      loss_scale_manager=loss_scale_manager)\n",
    "if device_num > 1:\n",
    "        ckpt_save_dir = os.path.join(config.ckpt_path + \"_\" + str(get_rank()))\n",
    "else:\n",
    "        ckpt_save_dir = config.ckpt_path\n",
    "time_cb = TimeMonitor(data_size=step_per_epoch)\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=config.save_checkpoint_steps,\n",
    "                                 keep_checkpoint_max=config.keep_checkpoint_max)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_alexnet\", directory=ckpt_save_dir, config=config_ck)\n",
    "\n",
    "print(\"============== Starting Training ==============\")\n",
    "model.train(config.epoch_size, ds_train, callbacks=[time_cb, ckpoint_cb, LossMonitor()],\n",
    "                dataset_sink_mode=config.dataset_sink_mode, sink_size=config.sink_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965433b9-f413-4f7c-a57e-d9d77f7bfe0d",
   "metadata": {},
   "source": [
    "## 训练完毕\n",
    "### 我们探究了AlexNet中不同Momentum值对于loss的影响，分别在yaml文件中更改Momentum的值得到以下结果\n",
    "\n",
    "![AlexNet]( alexnet.png \"AlexNet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada6340-8d33-4272-989c-85867fe00d8d",
   "metadata": {},
   "source": [
    "# 3.LetNet\n",
    "使用Minist数据来训练的经典网络网络模型LeNet，通过这个手写数据集的案例，让读者对MindSpore中的Momentum优化器有一个基础的认识，并且与SGD优化器做对比，理解Momentum与SGD优化器的不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae5039-d6de-4fa4-9043-23b8a41ecf8c",
   "metadata": {},
   "source": [
    "### 3.1 数据集简介\n",
    "数据集MNIST是一个手写数字图片数据集，共包含图像和对应的标签。\n",
    "数据集中所有图片都是28x28像素大小，且所有的图像都经过了适当的处理使得数字位于图片的中心位置。MNIST数据集使用二进制方式存储。图片数据中每个图片为一个长度为784（28x28x1，即长宽28像素的单通道灰度图）的一维向量，而标签数据中每个标签均为长度为10的一维向量。\n",
    "训练集一共包含了 60,000 张图像和标签，而测试集一共包含了 10,000 张图像和标签。\n",
    "数据集文件的目录结构如下：\n",
    "```text\n",
    "./mnist/\n",
    "├── test\n",
    "│   ├── t10k-images-idx3-ubyte\n",
    "│   └── t10k-labels-idx1-ubyte\n",
    "└── train\n",
    "     ├── train-images-idx3-ubyte\n",
    "     └── train-labels-idx1-ubyte\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d3e4b-137a-4a4c-b828-3f5cf945b2b7",
   "metadata": {},
   "source": [
    "### 3.2 LeNet网络模型简介\n",
    "![image.png](letnet.png)\n",
    "\n",
    "上图就是LeNet的网络结构，LeNet又被称为LeNet-5，其之所以称为这个名称是由于原始的LeNet是一个5层的卷积神经网络，它主要包括两部分：\n",
    "* 卷积层\n",
    "* 全连接层\n",
    "\n",
    "其中卷积层数为2，全连接层数为3。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38b075-061d-4b7a-8bd1-0eeea2632d8a",
   "metadata": {},
   "source": [
    "###  实验步骤\n",
    "#### 下载并处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5178e379-e80f-4aa0-b047-f9101956986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344B [00:02, 4925623.68B/s]                               \n",
      "29696B [00:00, 35699068.96B/s]          \n",
      "9913344B [00:00, 20424808.09B/s]                              \n",
      "29696B [00:00, 33527335.55B/s]          \n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.339.963 [mindspore/dataset/core/validator_helpers.py:804] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.343.148 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.344.357 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.345.416 [mindspore/dataset/core/validator_helpers.py:804] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.347.807 [mindspore/dataset/core/validator_helpers.py:804] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.348.795 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.349.716 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.350.620 [mindspore/dataset/core/validator_helpers.py:804] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.353.084 [mindspore/dataset/core/validator_helpers.py:804] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.354.051 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.354.964 [mindspore/dataset/core/validator_helpers.py:804] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(19425:281473119143808,MainProcess):2022-11-08-11:06:36.355.851 [mindspore/dataset/core/validator_helpers.py:804] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    }
   ],
   "source": [
    "from mindvision.dataset import Mnist\n",
    "download_train_M = Mnist(path=\"./mnist\\\\train_M\\\\train-images-idx3-ubyte\", split=\"train\", batch_size=32, repeat_num=1, shuffle=True, resize=32, download=True)\n",
    "download_train_S = Mnist(path=\"./mnist\\\\train_S\\\\train-images-idx3-ubyte\", split=\"train\", batch_size=32, repeat_num=1, shuffle=True, resize=32, download=True)\n",
    "\n",
    "download_eval = Mnist(path=\"./mnist\\\\test\\\\train-images-idx3-ubyte\", split=\"test\", batch_size=32, resize=32, download=True)\n",
    "\n",
    "dataset_train_Momentum = download_train_M.run()\n",
    "dataset_train_SGD = download_train_S.run()\n",
    "dataset_eval = download_eval.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7efd40-cb55-43d1-a907-b000b02193ba",
   "metadata": {},
   "source": [
    "###  创建网络模型\n",
    "因为MindSpore Vision套件提供了LeNet网络模型接口`lenet`，所以这这里我们就可以不必自己去创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db019bdd-0458-486e-b327-3d22c903ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.classification.models import lenet\n",
    "\n",
    "network = lenet(num_classes=10, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a0b04-3ecf-4d55-8225-f5762a849ce3",
   "metadata": {},
   "source": [
    "#### 定义损失函数和优化器\n",
    "其中损失函数采用交叉熵损失函数`SoftmaxCrossEntropyWithLogits`   \n",
    "优化器使用`Momentum`，并使用`SGD`函数作为对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939f28f4-a9f8-436f-bc02-8a84438f7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "# 定义损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# 定义优化器函数\n",
    "net_opt_Momentum = nn.Momentum(network.trainable_params(), learning_rate=0.01,momentum=0.9)\n",
    "net_opt_SGD = nn.SGD(network.trainable_params(), learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b084463-582e-47d4-a448-90d400196e82",
   "metadata": {},
   "source": [
    "#### 训练并保存模型\n",
    "通过MindSpore提供的`model.train`接口可以方便地进行网络的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148ed200-a9f3-4ea3-9825-5d95b07f7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "\n",
    "# 设置模型保存参数，模型训练保存参数的step为1875。\n",
    "config_ck = ms.CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "\n",
    "# 应用模型保存参数\n",
    "ckpoint = ms.ModelCheckpoint(prefix=\"lenet\", directory=\"./lenet\", config=config_ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502d03e-6473-4983-8ac1-7d64e0107063",
   "metadata": {},
   "source": [
    "通过MindSpore提供的`model.train`接口可以方便地进行网络的训练，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7accd8-482e-4a54-8a3d-5a3555850042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(19425,fffe427fc1e0,python):2022-11-08-11:07:17.710.036 [mindspore/ccsrc/minddata/dataset/engine/datasetops/device_queue_op.cc:752] DetectPerBatchTime] Bad performance attention, it takes more than 25 seconds to fetch a batch of data from dataset pipeline, which might result `GetNext` timeout problem. You may test dataset processing performance(with creating dataset iterator) and optimize it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[  0/ 10], step:[ 1875/ 1875], loss:[0.303/0.303], time:34333.182 ms, lr:0.01000\n",
      "Epoch time: 34387.724 ms, per step time: 18.340 ms, avg loss: 0.303\n",
      "Epoch:[  1/ 10], step:[ 1875/ 1875], loss:[0.028/0.028], time:3556.259 ms, lr:0.01000\n",
      "Epoch time: 3561.532 ms, per step time: 1.899 ms, avg loss: 0.028\n",
      "Epoch:[  2/ 10], step:[ 1875/ 1875], loss:[0.003/0.003], time:3961.307 ms, lr:0.01000\n",
      "Epoch time: 3967.488 ms, per step time: 2.116 ms, avg loss: 0.003\n",
      "Epoch:[  3/ 10], step:[ 1875/ 1875], loss:[0.021/0.021], time:3256.912 ms, lr:0.01000\n",
      "Epoch time: 3264.241 ms, per step time: 1.741 ms, avg loss: 0.021\n",
      "Epoch:[  4/ 10], step:[ 1875/ 1875], loss:[0.009/0.009], time:3349.514 ms, lr:0.01000\n",
      "Epoch time: 3352.748 ms, per step time: 1.788 ms, avg loss: 0.009\n",
      "Epoch:[  5/ 10], step:[ 1875/ 1875], loss:[0.011/0.011], time:3340.017 ms, lr:0.01000\n",
      "Epoch time: 3342.766 ms, per step time: 1.783 ms, avg loss: 0.011\n",
      "Epoch:[  6/ 10], step:[ 1875/ 1875], loss:[0.138/0.138], time:3844.825 ms, lr:0.01000\n",
      "Epoch time: 3852.520 ms, per step time: 2.055 ms, avg loss: 0.138\n",
      "Epoch:[  7/ 10], step:[ 1875/ 1875], loss:[0.032/0.032], time:4469.832 ms, lr:0.01000\n",
      "Epoch time: 4477.036 ms, per step time: 2.388 ms, avg loss: 0.032\n",
      "Epoch:[  8/ 10], step:[ 1875/ 1875], loss:[0.003/0.003], time:5056.273 ms, lr:0.01000\n",
      "Epoch time: 5059.906 ms, per step time: 2.699 ms, avg loss: 0.003\n",
      "Epoch:[  9/ 10], step:[ 1875/ 1875], loss:[0.002/0.002], time:4061.783 ms, lr:0.01000\n",
      "Epoch time: 4064.060 ms, per step time: 2.167 ms, avg loss: 0.002\n",
      "Epoch:[  0/ 10], step:[ 1875/ 1875], loss:[0.003/0.003], time:9446.492 ms, lr:0.01000\n",
      "Epoch time: 9510.634 ms, per step time: 5.072 ms, avg loss: 0.003\n",
      "Epoch:[  1/ 10], step:[ 1875/ 1875], loss:[0.001/0.001], time:4228.277 ms, lr:0.01000\n",
      "Epoch time: 4235.755 ms, per step time: 2.259 ms, avg loss: 0.001\n",
      "Epoch:[  2/ 10], step:[ 1875/ 1875], loss:[0.001/0.001], time:4310.635 ms, lr:0.01000\n",
      "Epoch time: 4319.320 ms, per step time: 2.304 ms, avg loss: 0.001\n",
      "Epoch:[  3/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:4318.736 ms, lr:0.01000\n",
      "Epoch time: 4325.073 ms, per step time: 2.307 ms, avg loss: 0.000\n",
      "Epoch:[  4/ 10], step:[ 1875/ 1875], loss:[0.003/0.003], time:4369.116 ms, lr:0.01000\n",
      "Epoch time: 4374.083 ms, per step time: 2.333 ms, avg loss: 0.003\n",
      "Epoch:[  5/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:4271.743 ms, lr:0.01000\n",
      "Epoch time: 4278.739 ms, per step time: 2.282 ms, avg loss: 0.000\n",
      "Epoch:[  6/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:4249.203 ms, lr:0.01000\n",
      "Epoch time: 4252.615 ms, per step time: 2.268 ms, avg loss: 0.000\n",
      "Epoch:[  7/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:4147.828 ms, lr:0.01000\n",
      "Epoch time: 4150.343 ms, per step time: 2.214 ms, avg loss: 0.000\n",
      "Epoch:[  8/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:3926.698 ms, lr:0.01000\n",
      "Epoch time: 3931.716 ms, per step time: 2.097 ms, avg loss: 0.000\n",
      "Epoch:[  9/ 10], step:[ 1875/ 1875], loss:[0.000/0.000], time:4114.315 ms, lr:0.01000\n",
      "Epoch time: 4117.425 ms, per step time: 2.196 ms, avg loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "from mindvision.engine.callback import LossMonitor\n",
    "import mindspore as ms\n",
    "\n",
    "# 初始化模型参数\n",
    "model_Momentum = ms.Model(network, loss_fn=net_loss, optimizer=net_opt_Momentum, metrics={'accuracy'})\n",
    "# 训练网络模型，并保存为lenet-1_1875.ckpt文件\n",
    "model_Momentum.train(10, dataset_train_Momentum, callbacks=[ckpoint, LossMonitor(0.01, 1875)])\n",
    "\n",
    "#使用SGD作为对比\n",
    "model_SGD=ms.Model(network, loss_fn=net_loss, optimizer=net_opt_SGD, metrics={'accuracy'})\n",
    "model_SGD.train(10, dataset_train_SGD, callbacks=[ckpoint, LossMonitor(0.01, 1875)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8133c52-477b-4866-aa43-c06d3aaf114a",
   "metadata": {},
   "source": [
    "#### 结论\n",
    "可以看出momentum优化器相较于SGD优化器有着更好的性能，使用同样训练集的情况下，momentum的loss值下降更快，更容易到达一个相对稳定的阶段。\n",
    "![image.png](letnet_R.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d2b09-9e6e-4a2d-b3b4-bf6cbfe33f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
