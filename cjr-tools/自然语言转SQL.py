# -*- coding: utf-8 -*-
"""building.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rlWcT1HRea6TlHLMxh6Qq6MbzN7Krr4M
"""

class LangChainCFG:
    llm_model_name = 'THUDM/chatglm-6b-int4'  # 本地模型文件 or huggingface远程仓库
    embedding_model_name = 'GanymedeNil/text2vec-large-chinese'  # 检索模型文件 or huggingface远程仓库
    vector_store_path = '.'
    docs_path = './docs'

!pip install accelerate

!pip install langchain

!pip install transformers

!pip install sentencepiece

import os
from typing import Dict, Union, Optional
from typing import List

from accelerate import load_checkpoint_and_dispatch
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from transformers import AutoModel, AutoTokenizer
"""
LLM 是大型语言模型（Large Language Models）的缩写。
它是 LangChain 的核心组件，
LangChain 平台提供了与各种 LLM 提供商
（如 OpenAI、Cohere 和 Hugging Face）交互的标准接口。
LLM 用于根据输入提示生成文本，可用于语言翻译、文本补全和问题解答等多种任务。
它们是自然语言处理的强大工具，可通过 LangChain 平台访问。
"""

class ChatGLMService(LLM):
    max_token: int = 10000  #最大句子数
    temperature: float = 0.1 # 可变度
    top_p = 0.9 # 最大可变度
    history = []  # 历史
    tokenizer: object = None
    model: object = None

    def __init__(self):
        super().__init__()
    @property
    def _llm_type(self) -> str:
        return "ChatGLM"

    def _call(self,
              prompt: str,
              stop: Optional[List[str]] = None) -> str:
        response, _ = self.model.chat(
            self.tokenizer,
            prompt,
            history=self.history,
            max_length=self.max_token,
            temperature=self.temperature,
        )
        if stop is not None:
            response = enforce_stop_tokens(response, stop)
        self.history = self.history + [[None, response]]
        return response

    def load_model(self,
                   model_name_or_path: str = "THUDM/chatglm-6b"):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half().cuda()
        self.model = self.model.eval()

    def auto_configure_device_map(self, num_gpus: int) -> Dict[str, int]:
        # transformer.word_embeddings 占用1层
        # transformer.final_layernorm 和 lm_head 占用1层
        # transformer.layers 占用 28 层
        # 总共30层分配到num_gpus张卡上
        num_trans_layers = 28
        per_gpu_layers = 30 / num_gpus

        # bugfix: 在linux中调用torch.embedding传入的weight,input不在同一device上,导致RuntimeError
        # windows下 model.device 会被设置成 transformer.word_embeddings.device
        # linux下 model.device 会被设置成 lm_head.device
        # 在调用chat或者stream_chat时,input_ids会被放到model.device上
        # 如果transformer.word_embeddings.device和model.device不同,则会导致RuntimeError
        # 因此这里将transformer.word_embeddings,transformer.final_layernorm,lm_head都放到第一张卡上
        device_map = {'transformer.word_embeddings': 0,
                      'transformer.final_layernorm': 0, 'lm_head': 0}

        used = 2
        gpu_target = 0
        for i in range(num_trans_layers):
            if used >= per_gpu_layers:
                gpu_target += 1
                used = 0
            assert gpu_target < num_gpus
            device_map[f'transformer.layers.{i}'] = gpu_target
            used += 1

        return device_map

    def load_model_on_gpus(self, model_name_or_path: Union[str, os.PathLike], num_gpus: int = 2,
                           multi_gpu_model_cache_dir: Union[str, os.PathLike] = "./temp_model_dir",
                           ):
        # https://github.com/THUDM/ChatGLM-6B/issues/200
        self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, )
        self.model = self.model.eval()

        device_map = self.auto_configure_device_map(num_gpus)
        try:
            self.model = load_checkpoint_and_dispatch(
                self.model, model_name_or_path, device_map=device_map, offload_folder="offload",
                offload_state_dict=True).half()
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name_or_path,
                trust_remote_code=True
            )
        except ValueError:
            # index.json not found
            print(f"index.json not found, auto fixing and saving model to {multi_gpu_model_cache_dir} ...")

            assert multi_gpu_model_cache_dir is not None, "using auto fix, cache_dir must not be None"
            self.model.save_pretrained(multi_gpu_model_cache_dir, max_shard_size='2GB')
            self.model = load_checkpoint_and_dispatch(
                self.model, multi_gpu_model_cache_dir, device_map=device_map,
                offload_folder="offload", offload_state_dict=True).half()
            self.tokenizer = AutoTokenizer.from_pretrained(
                multi_gpu_model_cache_dir,
                trust_remote_code=True
            )
            print(f"loading model successfully, you should use checkpoint_path={multi_gpu_model_cache_dir} next time")

config = LangChainCFG()
llm_service = ChatGLMService()
llm_service.load_model(model_name_or_path=config.llm_model_name)

!pip install neo4j
!pip install py2neo

import os
from langchain.indexes import VectorstoreIndexCreator
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.chains import SimpleSequentialChain
from langchain.chains import SequentialChain
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser
from langchain.prompts import PromptTemplate
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
import pandas as pd
import neo4j
from langchain.chat_models import ChatOpenAI
from langchain.chains import GraphCypherQAChain
from langchain.graphs import Neo4jGraph
import os
from py2neo import Graph, Node

graph = Neo4jGraph(
    url="neo4j+s://9497e0d5.databases.neo4j.io", username="neo4j", password="qLW1P0HXTqI9sVjqUlvHMeoAJj9Fyx-Tqt5GirPYQhc"
)

graph.refresh_schema() #如果数据发生变化，刷新

from langchain.schema import (
    BaseLLMOutputParser,
    BasePromptTemplate,
    LLMResult,
    PromptValue,
    StrOutputParser,
)

from langchain.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT

qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT
cypher_prompt: BasePromptTemplate = CYPHER_GENERATION_PROMPT
qa_chain = LLMChain(llm=llm_service, prompt=qa_prompt)
cypher_generation_chain = LLMChain(llm=llm_service, prompt=cypher_prompt)

!pip install cpm_kernels

import torch
question="李白写的诗"
from langchain.callbacks.manager import CallbackManagerForChainRun
_run_manager = CallbackManagerForChainRun.get_noop_manager()
callbacks = _run_manager.get_child()
generated_cypher = cypher_generation_chain.run(
        {"question": question, "schema": graph.get_schema}, callbacks=callbacks
        )

print(generated_cypher)

import re
def extract_cypher(text: str) -> str:
    """Extract Cypher code from a text.

    Args:
        text: Text to extract Cypher code from.

    Returns:
        Cypher code extracted from the text.
    """
    # The pattern to find Cypher code enclosed in triple backticks
    pattern = r"```(.*?)```"

    # Find all matches in the input text
    matches = re.findall(pattern, text, re.DOTALL)

    return matches[0] if matches else text

# 提取 Cypher code if it is 如果它被反斜线包裹
generated_cypher = extract_cypher(generated_cypher)

_run_manager.on_text("Generated Cypher:", end="\n",verbose=True)
_run_manager.on_text(
    generated_cypher, color="green", end="\n",verbose=True
    )

_run_manager.on_text

intermediate_steps = []
intermediate_steps.append({"query": generated_cypher})

print(intermediate_steps)

# Retrieve and limit the number of results
context = graph.query(generated_cypher)