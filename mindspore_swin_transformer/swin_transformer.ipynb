{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于MindSpore框架的Swin Transformer案例实现\n",
    "\n",
    "## 模型简介\n",
    "\n",
    "### 模型背景\n",
    "\n",
    "Swin Transfromer在2021年首次发表于论文《Swin Transformer: Hierarchical Vision Transformer using Shifted Windows》，目前已用于图像分割、分类等计算机视觉领域的各项任务中。该模型借鉴了Vision Transformer模型的思想，将二维图像加工成transformer可处理的一维数据，试图将transformer这一自然语言处理领域的利器，迁移至计算机视觉领域，以获得较优的模型性能。\n",
    "\n",
    "目前，transformer应用到CV领域，需要克服一些难题：\n",
    "\n",
    "- 随着图像的分辨率增加，展平后的序列长度呈平方级别增加，是模型不可接受的，将严重影响transformer中自注意力的计算效率；\n",
    "- 不同于NLP模型的传统输入特征，同一物体的图像因拍摄角度等原因，尺度和内容特征方差较大。\n",
    "\n",
    "Swin Transformer创新地引入了滑动窗口机制，在窗口内进行自注意力计算，使计算复杂度随图片分辨率平方级别增长降低为线性增长，并使模型可以学习到跨窗口的图像信息；参考了传统的CNN结构，进行类似的级联式局部特征提取工作，在每一层次进行下采样以扩大感受野，并可提取到多尺度的图像信息。\n",
    "\n",
    "### 模型基本结构\n",
    "\n",
    "Swin Transfromer的基本结构如图1(d)所示，由4个层级式模块组成，每一模块内包含一个Swin Transformer Block。原始输入图像尺寸为H×W×3（3为RGB通道数），经过Patch Partition层分为大小为4×4的patch，尺寸转换为(H/4)×(W/4)×48，其功能等价于卷积核大小为4×4，步长为4，卷积核个数为48的二维卷积操作。\n",
    "\n",
    "随后，每个Stage内部由Patch Merging模块（Stage1为Linear Embedding）、Swin Transformer模块组成。以Stage1、Stage2为例：\n",
    "\n",
    "- Linear Embedding\n",
    "\n",
    "  线性嵌入模块（Linear Embedding）将图像的通道数调整为嵌入长度C，调整后的图像尺寸为(H/4)×(W/4)×C。\n",
    "\n",
    "- Swin Transformer\n",
    "\n",
    "  Linear Embedding模块的输出序列长度往往较大，无法直接输入transformer。在本模型中，将输入张量划分为m×m大小的窗口，其中m为每个窗口内的patch数量，原文模型中默认为7。自注意力计算将在每个窗口内展开。为提取窗口间的信息，对窗口进行滑动，并再次计算自注意力。有关Swin Transformer模块的自注意力计算等实现细节见下文。\n",
    "\n",
    "- Patch Merging\n",
    "\n",
    "  下采样模块（Patch Merging）的作用是降低图片分辨率，扩大感受野，捕获多尺寸的图片信息，同时降低计算量，如图1(a)所示。类比于下采样在CNN模型中的实现，Swin Transformer模块的输出经过一次下采样，由(H/4)×(W/4)×C转换为(H/8)×(W/8)×2C，将临近的2个小patch合并成一个大patch，相当于进行了步长为2的二维卷积操作，后经过线性层（或1×1卷积层）调整通道数至2C，功能等价于Patch Partition模块与Linear Embedding模块的先后组合。此后再经过多个Swin Transformer模块、Patch Merging模块，进行多尺度特征的提取。\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://github.com/microsoft/Swin-Transformer/blob/main/figures/teaser.png?raw=true\" alt=\"image-20220819101847606\" style=\"zoom:67%;\" />\n",
    "    <br>\n",
    "    <div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 3px;\">\n",
    "    <center> 图1 Swin Transformer模型结构图 </center> </div>\n",
    "</center>\n",
    "\n",
    "### Swin Transformer模块原理\n",
    "\n",
    "Swin Transformer模块的细节实现如图1(c)所示，核心部分为多头自注意力的计算。如上所述，基于全尺寸图像的自注意力在面对密集计算型任务时具有较高的计算复杂度，因此本模型采用基于窗口的自注意力计算（W-MSA），输入张量先被分割成窗口，在窗口内的patch之间计算多头自注意力。相比于传统的自注意力计算，此处在计算q、k时额外增加了相对位置编码，以提供窗口内的位置信息。\n",
    "\n",
    "Swin Transformer的亮点在于采用滑动窗口机制，提取窗口之间的信息，达到全局自注意力的效果。以4个4×4大小的窗口为例，在计算滑动窗口多头自注意力（SW-MSA）时，将窗口向右下移动2个patch，得到9个新窗口，其中上、下、左、右4个新窗口包含原窗口划分中2个窗口的图像信息，中间的新窗口包含原窗口划分中4个窗口的图像信息，从而实现窗口之间的通信。但随之而来的是窗口数量的成倍增加和不同的窗口大小，反而增大了计算难度。\n",
    "\n",
    "为降低计算量，本模型对新窗口进行循环移位处理，组成新的窗口布局，将4×4大小的窗口数降低为4个，如图1(b)所示。此时除左上角的窗口外，其它3个窗口包含的部分patch原本并不属于同一区域，不应计算之间的自注意力，因此本模型创新性地提出了masked MSA机制，原理在自注意力计算结果上加上mask矩阵，目的是只取相同原窗口内的patch间的q、k计算结果，而不同原窗口内的patch间q、k计算结果添加原码后得到一个较大的负数，在随后的softmax层计算中，掩码部分输出将会是0，达到忽略其值的目的。\n",
    "\n",
    "### 模型特点\n",
    "\n",
    "- 基于滑动窗口的自注意力计算，交替使用W-MSA、SW-MSA，解决transformer应用于计算机视觉领域所产生的计算复杂度高的问题；\n",
    "- 借鉴CNN架构，对图像进行降采样，捕获多尺寸层次的全局特征。\n",
    "\n",
    "## 仓库说明\n",
    "\n",
    "仓库结构为：  \n",
    "```text\n",
    "└─ swin_transformer\n",
    "    ├─ src\n",
    "        ├─ configs                // SwinTransformer的配置文件\n",
    "            ├─ args.py\n",
    "            └─ swin_tiny_patch4_window7_224.yaml\n",
    "        └─ data\n",
    "            ├─ augment            // 数据增强函数文件\n",
    "                ├─ auto_augment.py\n",
    "                ├─ mixup.py\n",
    "                └─ random_erasing.py\n",
    "            └─ imagenet           // miniImageNet数据集\n",
    "                ├─ train\n",
    "                ├─ val\n",
    "                └─ test\n",
    "    ├─ swin_transformer.ipynb     // 端到端可执行的Notebook文件\n",
    "    └─ README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MindSpore相关依赖引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mindspore.common.initializer as weight_init\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore import Parameter\n",
    "from mindspore import Tensor\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore import nn\n",
    "from mindspore import numpy\n",
    "from mindspore import ops\n",
    "\n",
    "import os\n",
    "import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as C\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore.dataset.vision.utils import Inter\n",
    "\n",
    "import collections.abc\n",
    "from itertools import repeat\n",
    "\n",
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore import Model\n",
    "from mindspore import context\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor, Callback\n",
    "from mindspore.nn.loss.loss import LossBase\n",
    "from mindspore.common import RowTensor\n",
    "from mindspore.ops import composite as Cps\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.communication.management import init, get_rank\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.nn.optim import AdamWeightDecay\n",
    "from mindspore.nn.optim.momentum import Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinTransformer模型定义\n",
    "\n",
    "如下所示，代码中SwinTransformer类的定义与继承结构和原文中的模型原理结构具有对应关系：  \n",
    "```text\n",
    "SwinTransformer\n",
    "    ├─ PatchEmbed\n",
    "    └─ BasicLayer\n",
    "        ├─ PatchMerging\n",
    "        └─ SwinTransformerBlock\n",
    "            ├─ WindowAttention\n",
    "            ├─ RelativeBias\n",
    "            ├─ DropPath1D\n",
    "            ├─ Mlp\n",
    "            ├─ Roll\n",
    "            ├─ WindowPartitionConstruct\n",
    "            └─ WindowReverseConstruct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reading YAML config from ./src/configs/swin_tiny_patch4_window7_224.yaml\n",
      "Namespace(arch='swin_tiny_patch4_window7_224', accumulation_step=1, amp_level='O1', ape=False, batch_size=128, beta=[0.9, 0.999], clip_global_norm_value=5.0, crop=True, data_url='./src/data/imagenet', device_id=1, device_num=1, device_target='GPU', epochs=100, eps=1e-08, file_format='MINDIR', in_channel=3, is_dynamic_loss_scale=True, keep_checkpoint_max=20, optimizer='adamw', set='ImageNet', graph_mode=0, mix_up=0.8, mlp_ratio=4.0, num_parallel_workers=16, start_epoch=0, warmup_length=20, warmup_lr=7e-08, weight_decay=0.05, loss_scale=1024, lr=0.0005, lr_scheduler='cosine_lr', lr_adjust=30, lr_gamma=0.97, momentum=0.9, num_classes=100, patch_size=4, patch_norm=True, swin_config='./src/configs/swin_tiny_patch4_window7_224.yaml', seed=0, save_every=5, label_smoothing=0.1, image_size=224, train_url='./', cutmix=1.0, auto_augment='rand-m9-mstd0.5-inc1', interpolation='bicubic', re_prob=0.25, re_mode='pixel', re_count=1, mixup_prob=1.0, switch_prob=0.5, mixup_mode='batch', base_lr=0.0005, min_lr=6e-06, nonlinearity='GELU', keep_bn_fp32=True, drop_path_rate=0.2, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7)\n"
     ]
    }
   ],
   "source": [
    "from src.configs.args import args\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "class Identity(nn.Cell):\n",
    "    \"\"\"Identity\"\"\"\n",
    "    def construct(self, x):\n",
    "        return x\n",
    "\n",
    "class DropPath(nn.Cell):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob, ndim):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop = nn.Dropout(keep_prob=1 - drop_prob)\n",
    "        shape = (1,) + (1,) * (ndim + 1)\n",
    "        self.ndim = ndim\n",
    "        self.mask = Tensor(np.ones(shape), dtype=mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = ops.Tile()(self.mask, (x.shape[0],) + (1,) * (self.ndim + 1))\n",
    "        out = self.drop(mask)\n",
    "        out = out * x\n",
    "        return out\n",
    "\n",
    "\n",
    "class DropPath1D(DropPath):\n",
    "    def __init__(self, drop_prob):\n",
    "        super(DropPath1D, self).__init__(drop_prob=drop_prob, ndim=1)\n",
    "\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "act_layers = {\n",
    "    \"GELU\": nn.GELU,\n",
    "    \"gelu\": nn.GELU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Cell):\n",
    "    \"\"\"MLP Cell\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=act_layers[args.nonlinearity],\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n",
    "        self.drop = nn.Dropout(keep_prob=1.0 - drop)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = np.reshape(x, (B, H // window_size, window_size, W // window_size, window_size, C))\n",
    "    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowPartitionConstruct(nn.Cell):\n",
    "    \"\"\"WindowPartitionConstruct Cell\"\"\"\n",
    "\n",
    "    def __init__(self, window_size):\n",
    "        super(WindowPartitionConstruct, self).__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H, W, C)\n",
    "            window_size (int): window size\n",
    "\n",
    "        Returns:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        x = P.Reshape()(x, (B, H // self.window_size, self.window_size, W // self.window_size, self.window_size, C))\n",
    "        x = P.Transpose()(x, (0, 1, 3, 2, 4, 5))\n",
    "        x = P.Reshape()(x, (B * H * W // (self.window_size ** 2), self.window_size, self.window_size, C))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowReverseConstruct(nn.Cell):\n",
    "    \"\"\"WindowReverseConstruct Cell\"\"\"\n",
    "\n",
    "    def construct(self, windows, window_size, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "            window_size (int): Window size\n",
    "            H (int): Height of image\n",
    "            W (int): Width of image\n",
    "\n",
    "        Returns:\n",
    "            x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        B = windows.shape[0] // (H * W // window_size // window_size)\n",
    "        x = ops.Reshape()(windows, (B, H // window_size, W // window_size, window_size, window_size, -1))\n",
    "        x = ops.Transpose()(x, (0, 1, 3, 2, 4, 5))\n",
    "        x = ops.Reshape()(x, (B, H, W, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeBias(nn.Cell):\n",
    "    \"\"\"RelativeBias Cell\"\"\"\n",
    "\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super(RelativeBias, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        # define a parameter table of relative position bias\n",
    "        coords_h = np.arange(self.window_size[0]).reshape(self.window_size[0], 1).repeat(self.window_size[0],\n",
    "                                                                                         1).reshape(1, -1)\n",
    "        coords_w = np.arange(self.window_size[1]).reshape(1, self.window_size[1]).repeat(self.window_size[1],\n",
    "                                                                                         0).reshape(1, -1)\n",
    "        coords_flatten = np.concatenate([coords_h, coords_w], axis=0)  # 2, Wh, Ww\n",
    "        relative_coords = coords_flatten[:, :, np.newaxis] - coords_flatten[:, np.newaxis, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.transpose(1, 2, 0)  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        self.relative_position_index = Tensor(relative_coords.sum(-1).reshape(-1))  # Wh*Ww, Wh*Ww\n",
    "        self.relative_position_bias_table = Parameter(\n",
    "            Tensor(np.random.randn((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads),\n",
    "                   dtype=mstype.float32))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        self.one_hot = nn.OneHot(axis=-1, depth=(2 * window_size[0] - 1) * (2 * window_size[1] - 1),\n",
    "                                 dtype=mstype.float32)\n",
    "        self.index = Parameter(self.one_hot(self.relative_position_index), requires_grad=False)\n",
    "\n",
    "    def construct(self, axis=0):\n",
    "        out = ops.MatMul()(self.index, self.relative_position_bias_table)\n",
    "        out = P.Reshape()(out, (self.window_size[0] * self.window_size[1],\n",
    "                                self.window_size[0] * self.window_size[1], -1))\n",
    "        out = P.Transpose()(out, (2, 0, 1))\n",
    "        out = ops.ExpandDims()(out, 0)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Cell):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) Cell with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qZk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        if isinstance(dim, tuple) and len(dim) == 1:\n",
    "            dim = dim[0]\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = Tensor(qk_scale or head_dim ** -0.5, mstype.float32)\n",
    "        self.relative_bias = RelativeBias(self.window_size, num_heads)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(keep_prob=1.0 - attn_drop)\n",
    "        self.proj = nn.Dense(in_channels=dim, out_channels=dim, has_bias=True)\n",
    "        self.proj_drop = nn.Dropout(keep_prob=1.0 - proj_drop)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        q = ops.Reshape()(self.q(x), (B_, N, self.num_heads, C // self.num_heads)) * self.scale\n",
    "        q = ops.Transpose()(q, (0, 2, 1, 3))\n",
    "        k = ops.Reshape()(self.k(x), (B_, N, self.num_heads, C // self.num_heads))\n",
    "        k = ops.Transpose()(k, (0, 2, 3, 1))\n",
    "        v = ops.Reshape()(self.v(x), (B_, N, self.num_heads, C // self.num_heads))\n",
    "        v = ops.Transpose()(v, (0, 2, 1, 3))\n",
    "\n",
    "        attn = ops.BatchMatMul()(q, k)\n",
    "        attn = attn + self.relative_bias()\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[1]\n",
    "            attn = P.Reshape()(attn, (B_ // nW, nW, self.num_heads, N, N,)) + mask\n",
    "            attn = P.Reshape()(attn, (-1, self.num_heads, N, N,))\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = ops.Reshape()(ops.Transpose()(ops.BatchMatMul()(attn, v), (0, 2, 1, 3)), (B_, N, C))\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roll(nn.Cell):\n",
    "    \"\"\"Roll Cell\"\"\"\n",
    "\n",
    "    def __init__(self, shift_size, shift_axis=(1, 2)):\n",
    "        super(Roll, self).__init__()\n",
    "        self.shift_size = to_2tuple(shift_size)\n",
    "        self.shift_axis = shift_axis\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = numpy.roll(x, self.shift_size, self.shift_axis)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Cell):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Cell, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Cell, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=act_layers[args.nonlinearity], norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "\n",
    "        if isinstance(dim, int):\n",
    "            dim = (dim,)\n",
    "\n",
    "        self.norm1 = norm_layer(dim, epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath1D(drop_path) if drop_path > 0. else Identity()\n",
    "        self.norm2 = norm_layer(dim, epsilon=1e-5)\n",
    "        mlp_hidden_dim = int((dim[0] if isinstance(dim, tuple) else dim) * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim[0] if isinstance(dim, tuple) else dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = np.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            # img_mask: [1, 56, 56, 1] window_size: 7\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.reshape(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows[:, np.newaxis] - mask_windows[:, :, np.newaxis]\n",
    "            # [64, 49, 49] ==> [1, 64, 1, 49, 49]\n",
    "            attn_mask = np.expand_dims(attn_mask, axis=1)\n",
    "            attn_mask = np.expand_dims(attn_mask, axis=0)\n",
    "            attn_mask = Tensor(np.where(attn_mask == 0, 0., -100.), dtype=mstype.float32)\n",
    "            self.attn_mask = Parameter(attn_mask, requires_grad=False)\n",
    "            self.roll_pos = Roll(self.shift_size)\n",
    "            self.roll_neg = Roll(-self.shift_size)\n",
    "        else:\n",
    "            self.attn_mask = None\n",
    "\n",
    "        self.window_partition = WindowPartitionConstruct(self.window_size)\n",
    "        self.window_reverse = WindowReverseConstruct()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"construct function\"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, _, C = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = P.Reshape()(x, (B, H, W, C,))\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = self.roll_neg(x)\n",
    "            # shifted_x = numpy.roll(x, (-self.shift_size, -self.shift_size), (1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = self.window_partition(shifted_x)  # nW*B, window_size, window_size, C\n",
    "        x_windows = ops.Reshape()(x_windows,\n",
    "                                  (-1, self.window_size * self.window_size, C,))  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = P.Reshape()(attn_windows, (-1, self.window_size, self.window_size, C,))\n",
    "        shifted_x = self.window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = self.roll_pos(shifted_x)\n",
    "            # x = numpy.roll(shifted_x, (self.shift_size, self.shift_size), (1, 2))  # TODO:Don't stupid\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = P.Reshape()(x, (B, H * W, C,))\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Cell):\n",
    "    \"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim[0] if isinstance(dim, tuple) and len(dim) == 1 else dim\n",
    "        # Default False\n",
    "        self.reduction = nn.Dense(in_channels=4 * dim, out_channels=2 * dim, has_bias=False)\n",
    "        self.norm = norm_layer([dim * 4,])\n",
    "        self.H, self.W = self.input_resolution\n",
    "        self.H_2, self.W_2 = self.H // 2, self.W // 2\n",
    "        self.H2W2 = int(self.H * self.W // 4)\n",
    "        self.dim_mul_4 = int(dim * 4)\n",
    "        self.H2W2 = int(self.H * self.W // 4)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        x = P.Reshape()(x, (B, self.H_2, 2, self.W_2, 2, self.dim))\n",
    "        x = P.Transpose()(x, (0, 1, 3, 4, 2, 5))\n",
    "        x = P.Reshape()(x, (B, self.H2W2, self.dim_mul_4))\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Cell):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Cell, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Cell | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.CellList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,  # TODO: 这里window_size//2的时候特别慢\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"construct\"\"\"\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Cell):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        image_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        image_size = to_2tuple(image_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n",
    "                              pad_mode='pad', has_bias=True, weight_init=\"TruncatedNormal\")\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            if isinstance(embed_dim, int):\n",
    "                embed_dim = (embed_dim,)\n",
    "            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"docstring\"\"\"\n",
    "        B = x.shape[0]\n",
    "        # FIXME look at relaxing size constraints\n",
    "        x = ops.Reshape()(self.proj(x), (B, self.embed_dim, -1))  # B Ph*Pw C\n",
    "        x = ops.Transpose()(x, (0, 2, 1))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Cell):\n",
    "    \"\"\" Swin Transformer\n",
    "        A Pynp impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        image_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 100\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Cell): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=None, num_heads=None, window_size=7,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            image_size=image_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = Parameter(Tensor(np.zeros(1, num_patches, embed_dim), dtype=mstype.float32))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.CellList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer([self.num_features,], epsilon=1e-5)\n",
    "        self.avgpool = P.ReduceMean(keep_dims=False)\n",
    "        self.head = nn.Dense(in_channels=self.num_features,\n",
    "                             out_channels=num_classes, has_bias=True) if num_classes > 0 else Identity()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"init_weights\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Dense):\n",
    "                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n",
    "                                                             cell.weight.shape,\n",
    "                                                             cell.weight.dtype))\n",
    "                if isinstance(cell, nn.Dense) and cell.bias is not None:\n",
    "                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                               cell.bias.shape,\n",
    "                                                               cell.bias.dtype))\n",
    "            elif isinstance(cell, nn.LayerNorm):\n",
    "                cell.gamma.set_data(weight_init.initializer(weight_init.One(),\n",
    "                                                            cell.gamma.shape,\n",
    "                                                            cell.gamma.dtype))\n",
    "                cell.beta.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                           cell.beta.shape,\n",
    "                                                           cell.beta.dtype))\n",
    "\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(ops.Transpose()(x, (0, 2, 1)), 2)  # B C 1\n",
    "        return x\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## miniImageNet数据集引入\n",
    "\n",
    "所使用的数据集：miniImageNet  \n",
    "- 数据集大小：共100类，60000张图像，每类600张图像  \n",
    "- 数据格式：JPGE格式，84*84彩色图像  \n",
    "- 对数据集结构的处理要求：类别分布均衡，训练集 : 验证集 : 测试集 = 7 : 1 : 2  \n",
    "\n",
    "miniImageNet数据集的原始结构如下：  \n",
    "```text\n",
    "└─ dataset\n",
    "    ├─ images\n",
    "        ├─ n0153282900000005.jpg\n",
    "        ├─ n0153282900000006.jpg\n",
    "        ├─ ...\n",
    "    ├─ train.csv\n",
    "    ├─ val.csv\n",
    "    └─ test.csv\n",
    "```\n",
    "匹配图像与CSV文件后，数据集结构变为：  \n",
    "```text\n",
    "└─ dataset\n",
    "    ├─ train\n",
    "        ├─ 第1类\n",
    "            └─ 600张图像 \n",
    "        ├─ ...\n",
    "        └─ 第64类\n",
    "            └─ 600张图像 \n",
    "    ├─ val\n",
    "        ├─ 第65类\n",
    "            └─ 600张图像 \n",
    "        ├─ ...\n",
    "        └─ 第80类\n",
    "            └─ 600张图像 \n",
    "    └─ test\n",
    "        ├─ 第81类\n",
    "            └─ 600张图像 \n",
    "        ├─ ...\n",
    "        └─ 第100类\n",
    "            └─ 600张图像 \n",
    "```\n",
    "仍需进一步将数据集结构处理为：  \n",
    "```text\n",
    "└─ dataset\n",
    "    ├─ train\n",
    "        ├─ 第1类\n",
    "            └─ 420张图像 \n",
    "        ├─ ...\n",
    "        └─ 第100类\n",
    "            └─ 420张图像 \n",
    "    ├─ val\n",
    "        ├─ 第1类\n",
    "            └─ 60张图像 \n",
    "        ├─ ...\n",
    "        └─ 第100类\n",
    "            └─ 60张图像 \n",
    "    └─ test\n",
    "        ├─ 第1类\n",
    "            └─ 120张图像 \n",
    "        ├─ ...\n",
    "        └─ 第100类\n",
    "            └─ 120张图像 \n",
    "```\n",
    "处理后的数据集的下载方式：  \n",
    "- 链接：https://pan.baidu.com/s/14d6sWeZiZS8Hzua0ohw4BA 提取码：xqnu  \n",
    "\n",
    "请将数据集保存至路径“mindspore_swin_transformer/src/data”下，并逐层解压文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data operations, will be used in train.py and eval.py\n",
    "\"\"\"\n",
    "\n",
    "from src.data.augment.auto_augment import _pil_interp, rand_augment_transform\n",
    "from src.data.augment.mixup import Mixup\n",
    "from src.data.augment.random_erasing import RandomErasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rank_info():\n",
    "    \"\"\"\n",
    "    get rank size and rank id\n",
    "    \"\"\"\n",
    "    rank_size = int(os.environ.get(\"RANK_SIZE\", 1))\n",
    "\n",
    "    if rank_size > 1:\n",
    "        from mindspore.communication.management import get_rank, get_group_size\n",
    "        rank_size = get_group_size()\n",
    "        rank_id = get_rank()\n",
    "    else:\n",
    "        rank_size = rank_id = None\n",
    "\n",
    "    return rank_size, rank_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_imagenet(dataset_dir, args, repeat_num=1, training=True):\n",
    "    \"\"\"\n",
    "    create a train or val or test mini-imagenet dataset for SwinTransformer\n",
    "\n",
    "    Args:\n",
    "        dataset_dir(string): the path of dataset.\n",
    "        do_train(bool): whether dataset is used for train or eval.\n",
    "        repeat_num(int): the repeat times of dataset. Default: 1\n",
    "\n",
    "    Returns:\n",
    "        dataset\n",
    "    \"\"\"\n",
    "\n",
    "    device_num, rank_id = _get_rank_info()\n",
    "    shuffle = bool(training)\n",
    "    if device_num == 1 or not training:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers,\n",
    "                                         shuffle=shuffle)\n",
    "    else:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers, shuffle=shuffle,\n",
    "                                         num_shards=device_num, shard_id=rank_id)\n",
    "\n",
    "    image_size = args.image_size\n",
    "\n",
    "    # define map operations\n",
    "    # BICUBIC: 3\n",
    "    \n",
    "    # data augment\n",
    "    if training:\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        aa_params = dict(\n",
    "            translate_const=int(image_size * 0.45),\n",
    "            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n",
    "        )\n",
    "        interpolation = args.interpolation\n",
    "        auto_augment = args.auto_augment\n",
    "        assert auto_augment.startswith('rand')\n",
    "        aa_params['interpolation'] = _pil_interp(interpolation)\n",
    "\n",
    "        transform_img = [\n",
    "            vision.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(3 / 4, 4 / 3),\n",
    "                                          interpolation=Inter.BICUBIC),\n",
    "            vision.RandomHorizontalFlip(prob=0.5),\n",
    "            vision.ToPIL()\n",
    "        ]\n",
    "        transform_img += [rand_augment_transform(auto_augment, aa_params)]\n",
    "        transform_img += [\n",
    "            vision.ToTensor(),\n",
    "            vision.Normalize(mean=mean, std=std, is_hwc=False),\n",
    "            RandomErasing(args.re_prob, mode=args.re_mode, max_count=args.re_count)\n",
    "        ]\n",
    "    else:\n",
    "        mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "        std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "        # test transform complete\n",
    "        if args.crop:\n",
    "            transform_img = [\n",
    "                vision.Decode(),\n",
    "                vision.Resize(int(256 / 224 * image_size), interpolation=Inter.BICUBIC),\n",
    "                vision.CenterCrop(image_size),\n",
    "                vision.Normalize(mean=mean, std=std, is_hwc=True),\n",
    "                vision.HWC2CHW()\n",
    "            ]\n",
    "        else:\n",
    "            transform_img = [\n",
    "                vision.Decode(),\n",
    "                vision.Resize(int(image_size), interpolation=Inter.BICUBIC),\n",
    "                vision.Normalize(mean=mean, std=std, is_hwc=True),\n",
    "                vision.HWC2CHW()\n",
    "            ]\n",
    "\n",
    "    transform_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "    data_set = data_set.map(input_columns=\"image\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_img)\n",
    "    data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_label)\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.)  and not training:\n",
    "        # if use mixup and not training(False), one hot val data label\n",
    "        one_hot = C.OneHot(num_classes=args.num_classes)\n",
    "        data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                operations=one_hot)\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(args.batch_size, drop_remainder=True,\n",
    "                              num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.) and training:\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mix_up, cutmix_alpha=args.cutmix, cutmix_minmax=None,\n",
    "            prob=args.mixup_prob, switch_prob=args.switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.label_smoothing, num_classes=args.num_classes)\n",
    "\n",
    "        data_set = data_set.map(operations=mixup_fn, input_columns=[\"image\", \"label\"],\n",
    "                                num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    # apply dataset repeat operation\n",
    "    data_set = data_set.repeat(repeat_num)\n",
    "    ds.config.set_prefetch_size(4)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNet:\n",
    "    \"\"\"ImageNet Define\"\"\"\n",
    "\n",
    "    def __init__(self, args, training=True):\n",
    "        train_dir = os.path.join(args.data_url, \"train\")\n",
    "        val_ir = os.path.join(args.data_url, \"val\")\n",
    "        test_ir = os.path.join(args.data_url, \"test\")\n",
    "        if training:\n",
    "            self.train_dataset = create_dataset_imagenet(train_dir, training=True, args=args)\n",
    "            self.val_dataset = create_dataset_imagenet(val_ir, training=False, args=args)\n",
    "        self.test_dataset = create_dataset_imagenet(test_ir, training=False, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与评估  \n",
    "\n",
    "本案例利用MindSpore框架，基于Callback基于自定义了回调类 EvaluateCallBack，为了满足在模型训练时，能够在每一次epoch结束后计算3个评价指标：Loss、Top1-Acc、Top5-Acc，并根据评价指标的变化保存当前最优的模型的特定需求。  \n",
    "\n",
    "具体的实现流程首先是在EvaluateCallBack类中，通过__init__方法初始化相关参数，并重新实现epoch_end方法，在该方法中通过MindSpore的字典类型变量RunContext.original_args()获取模型训练时记录的相关属性，如cur_epoch_num，即当前epoch数，以及train_network,即训练的网络模型，在epoch_end方法中还计算了3个评价指标：Loss、Top1-Acc、Top5-Acc，并比较当前epoch的Top1-Acc值与记录的最优Top1-Acc值，利用先前获取到的train_network将表现最优的模型保存在特定路径下。综上实现了在模型训练时，每个epoch结束后计算eval_metrics类中定义的各个评价指标，并根据Top1-Acc指标保存最优模型。  \n",
    "\n",
    "在模型训练时，首先是设置模型训练的epoch次数为100，再通过自定义的create_dataset方法创建了训练集和验证集，设置batch_size大小为128，图像尺寸统一调整为224x224；损失函数使用SoftmaxCrossEntropyWithLogits计算预测值与真实值之间的交叉熵，优化器使用adamw，并设置学习率为0.005。回调函数方面使用了LossMonitor和TimeMonitor来监控训练过程中每个epoch结束后，损失值Loss的变化情况以及每个epoch、每个step的运行时间，还实例化了自定义的回调类EvaluateCallBack，实现计算每个epoch结束后，计算评价指标Loss、Top1-Acc和Top5-Acc，并保存当前最优模型。在100个epcoh结束后，模型在验证集和测试集上的评估指标为：  \n",
    "\n",
    "- 验证集：Top1-Acc：55.15%，Top5-Acc：81.00%\n",
    "- 测试集：Top1-Acc：55.26% ，Top5-Acc：81.59%\n",
    "\n",
    "本案例构建的网络模型具有较好的性能，能够实现对测试集进行较为准确的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置精度水平"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_keep_fp32(network, cell_types):\n",
    "    \"\"\"Cast cell to fp32 if cell in cell_types\"\"\"\n",
    "    for _, cell in network.cells_and_names():\n",
    "        if isinstance(cell, cell_types):\n",
    "            cell.to_float(mstype.float32)\n",
    "\n",
    "\n",
    "def cast_amp(net):\n",
    "    \"\"\"cast network amp_level\"\"\"\n",
    "    if args.amp_level == \"O1\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "        cell_types = (nn.GELU, nn.Softmax, nn.Conv2d, nn.Conv1d, nn.BatchNorm2d, nn.LayerNorm)\n",
    "        print(f\"=> cast {cell_types} to fp32 back\")\n",
    "        do_keep_fp32(net, cell_types)\n",
    "    elif args.amp_level == \"O2\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "        cell_types = (nn.BatchNorm2d, nn.LayerNorm)\n",
    "        print(f\"=> cast {cell_types} to fp32 back\")\n",
    "        do_keep_fp32(net, cell_types)\n",
    "    elif args.amp_level == \"O3\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "    else:\n",
    "        print(f\"=> using amp_level {args.amp_level}\")\n",
    "        args.loss_scale = 1.\n",
    "        args.is_dynamic_loss_scale = 0\n",
    "        print(f\"=> When amp_level is O0, using fixed loss_scale with {args.loss_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftTargetCrossEntropy(LossBase):\n",
    "    \"\"\"SoftTargetCrossEntropy for MixUp Augment\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftTargetCrossEntropy, self).__init__()\n",
    "        self.mean_ops = P.ReduceMean(keep_dims=False)\n",
    "        self.sum_ops = P.ReduceSum(keep_dims=False)\n",
    "        self.log_softmax = P.LogSoftmax()\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        logit = P.Cast()(logit, mstype.float32)\n",
    "        label = P.Cast()(label, mstype.float32)\n",
    "        loss = self.sum_ops(-label * self.log_softmax(logit), -1)\n",
    "        return self.mean_ops(loss)\n",
    "\n",
    "\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = P.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = Tensor(1.0 - smooth_factor, mstype.float32)\n",
    "        self.off_value = Tensor(1.0 * smooth_factor / (num_classes - 1), mstype.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "        self.cast = ops.Cast()\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, F.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss2 = self.ce(logit, label)\n",
    "        return loss2\n",
    "\n",
    "\n",
    "def get_criterion(args):\n",
    "    \"\"\"Get loss function from args.label_smooth and args.mix_up\"\"\"\n",
    "    assert args.label_smoothing >= 0. and args.label_smoothing <= 1.\n",
    "\n",
    "    if args.mix_up > 0. or args.cutmix > 0.:\n",
    "        print(25 * \"=\" + \"Using MixBatch\" + 25 * \"=\")\n",
    "        # smoothing is handled with mixup label transform\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "    elif args.label_smoothing > 0.:\n",
    "        print(25 * \"=\" + \"Using label smoothing\" + 25 * \"=\")\n",
    "        criterion = CrossEntropySmooth(sparse=True, reduction=\"mean\",\n",
    "                                       smooth_factor=args.label_smoothing,\n",
    "                                       num_classes=args.num_classes)\n",
    "    else:\n",
    "        print(25 * \"=\" + \"Using Simple CE\" + 25 * \"=\")\n",
    "        criterion = CrossEntropySmooth(sparse=True, reduction=\"mean\", num_classes=args.num_classes)\n",
    "\n",
    "    return criterion\n",
    "\n",
    "\n",
    "class NetWithLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "       NetWithLoss: Only support Network with Classfication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion):\n",
    "        super(NetWithLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        predict = self.model(data)\n",
    "        loss = self.criterion(predict, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义单步训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_grad_scale = Cps.MultitypeFuncGraph(\"grad_scale\")\n",
    "reciprocal = P.Reciprocal()\n",
    "\n",
    "@_grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def tensor_grad_scale(scale, grad):\n",
    "    return grad * F.cast(reciprocal(scale), F.dtype(grad))\n",
    "\n",
    "@_grad_scale.register(\"Tensor\", \"RowTensor\")\n",
    "def tensor_grad_scale_row_tensor(scale, grad):\n",
    "    return RowTensor(grad.indices,\n",
    "                     grad.values * F.cast(reciprocal(scale), F.dtype(grad.values)),\n",
    "                     grad.dense_shape)\n",
    "\n",
    "class TrainClipGrad(nn.TrainOneStepWithLossScaleCell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of SSD network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "        use_global_nrom(bool): Whether apply global norm before optimizer. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, optimizer,\n",
    "                 scale_sense=1.0, use_global_norm=True,\n",
    "                 clip_global_norm_value=1.0):\n",
    "        super(TrainClipGrad, self).__init__(network, optimizer, scale_sense)\n",
    "        self.use_global_norm = use_global_norm\n",
    "        self.clip_global_norm_value = clip_global_norm_value\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        \"\"\"construct\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(*inputs)\n",
    "        scaling_sens = self.scale_sense\n",
    "\n",
    "        status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n",
    "\n",
    "        scaling_sens_filled = Cps.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))\n",
    "        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n",
    "        grads = self.hyper_map(F.partial(_grad_scale, scaling_sens), grads)\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        # get the overflow buffer\n",
    "        cond = self.get_overflow_status(status, grads)\n",
    "        overflow = self.process_loss_scale(cond)\n",
    "        # if there is no overflow, do optimize\n",
    "        if not overflow:\n",
    "            if self.use_global_norm:\n",
    "                grads = Cps.clip_by_global_norm(grads, clip_norm=self.clip_global_norm_value)\n",
    "            loss = F.depend(loss, self.optimizer(grads))\n",
    "        else:\n",
    "            self.print(\"=============Over Flow, skipping=============\")\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_train_one_step(args, net_with_loss, optimizer):\n",
    "    \"\"\"get_train_one_step cell\"\"\"\n",
    "    if args.is_dynamic_loss_scale:\n",
    "        print(f\"=> Using DynamicLossScaleUpdateCell\")\n",
    "        scale_sense = nn.wrap.loss_scale.DynamicLossScaleUpdateCell(loss_scale_value=2 ** 24, scale_factor=2,\n",
    "                                                                    scale_window=2000)\n",
    "    else:\n",
    "        print(f\"=> Using FixedLossScaleUpdateCell, loss_scale_value:{args.loss_scale}\")\n",
    "        scale_sense = nn.wrap.FixedLossScaleUpdateCell(loss_scale_value=args.loss_scale)\n",
    "    net_with_loss = TrainClipGrad(net_with_loss, optimizer, scale_sense=scale_sense,\n",
    "                                  clip_global_norm_value=args.clip_global_norm_value,\n",
    "                                  use_global_norm=True)\n",
    "    return net_with_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义学习率、优化器和模型的获取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"multistep_lr\", \"cosine_lr\", \"constant_lr\", \"get_policy\", \"exp_lr\"]\n",
    "\n",
    "\n",
    "def get_policy(name):\n",
    "    \"\"\"get lr policy from name\"\"\"\n",
    "    if name is None:\n",
    "        return constant_lr\n",
    "\n",
    "    out_dict = {\n",
    "        \"constant_lr\": constant_lr,\n",
    "        \"cosine_lr\": cosine_lr,\n",
    "        \"multistep_lr\": multistep_lr,\n",
    "        \"exp_lr\": exp_lr,\n",
    "    }\n",
    "\n",
    "    return out_dict[name]\n",
    "\n",
    "\n",
    "def constant_lr(args, batch_num):\n",
    "    \"\"\"Get constant lr\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            lr = args.base_lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def exp_lr(args, batch_num):\n",
    "    \"\"\"Get exp lr \"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            lr = args.base_lr * args.lr_gamma ** epoch\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def cosine_lr(args, batch_num):\n",
    "    \"\"\"Get cosine lr\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            e = epoch - args.warmup_length\n",
    "            es = args.epochs - args.warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * args.base_lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def multistep_lr(args, batch_num):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        lr = args.base_lr * (args.lr_gamma ** (epoch / args.lr_adjust))\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def _warmup_lr(warmup_lr, base_lr, warmup_length, epoch):\n",
    "    \"\"\"Linear warmup\"\"\"\n",
    "    return epoch / warmup_length * (base_lr - warmup_lr) + warmup_lr\n",
    "\n",
    "def get_learning_rate(args, batch_num):\n",
    "    \"\"\"Get learning rate\"\"\"\n",
    "    return get_policy(args.lr_scheduler)(args, batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(args, model, batch_num):\n",
    "    \"\"\"Get optimizer for training\"\"\"\n",
    "    print(f\"=> When using train_wrapper, using optimizer {args.optimizer}\")\n",
    "    args.start_epoch = int(args.start_epoch)\n",
    "    optim_type = args.optimizer.lower()\n",
    "    params = get_param_groups(model)\n",
    "    learning_rate = get_learning_rate(args, batch_num)\n",
    "    step = int(args.start_epoch * batch_num)\n",
    "    accumulation_step = int(args.accumulation_step)\n",
    "    learning_rate = learning_rate[step::accumulation_step]\n",
    "    train_step = len(learning_rate)\n",
    "    print(f\"=> Get LR from epoch: {args.start_epoch}\\n\"\n",
    "          f\"=> Start step: {step}\\n\"\n",
    "          f\"=> Total step: {train_step}\\n\"\n",
    "          f\"=> Accumulation step:{accumulation_step}\")\n",
    "    learning_rate = learning_rate * args.batch_size * int(os.getenv(\"DEVICE_NUM\", args.device_num)) / 512.\n",
    "    if accumulation_step > 1:\n",
    "        learning_rate = learning_rate * accumulation_step\n",
    "\n",
    "    if optim_type == \"momentum\":\n",
    "        optim = Momentum(\n",
    "            params=params,\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    elif optim_type == \"adamw\":\n",
    "        optim = AdamWeightDecay(\n",
    "            params=params,\n",
    "            learning_rate=learning_rate,\n",
    "            beta1=args.beta[0],\n",
    "            beta2=args.beta[1],\n",
    "            eps=args.eps,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer {optim_type} is not supported\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "def get_param_groups(network):\n",
    "    \"\"\" get param groups \"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith(\".weight\"):\n",
    "            # Dense or Conv's weight using weight decay\n",
    "            decay_params.append(x)\n",
    "        else:\n",
    "            # all bias not using weight decay\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include LN\n",
    "            no_decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swintransformer(args):\n",
    "    \"\"\"get swintransformer according to args\"\"\"\n",
    "    # override args\n",
    "    image_size = args.image_size\n",
    "    patch_size = args.patch_size\n",
    "    in_chans = args.in_channel\n",
    "    embed_dim = args.embed_dim\n",
    "    depths = args.depths\n",
    "    num_heads = args.num_heads\n",
    "    window_size = args.window_size\n",
    "    drop_path_rate = args.drop_path_rate\n",
    "    mlp_ratio = args.mlp_ratio\n",
    "    qkv_bias = True\n",
    "    qk_scale = None\n",
    "    ape = args.ape\n",
    "    patch_norm = args.patch_norm\n",
    "    print(25 * \"=\" + \"MODEL CONFIG\" + 25 * \"=\")\n",
    "    print(f\"==> IMAGE_SIZE:         {image_size}\")\n",
    "    print(f\"==> PATCH_SIZE:         {patch_size}\")\n",
    "    print(f\"==> NUM_CLASSES:        {args.num_classes}\")\n",
    "    print(f\"==> EMBED_DIM:          {embed_dim}\")\n",
    "    print(f\"==> NUM_HEADS:          {num_heads}\")\n",
    "    print(f\"==> DEPTHS:             {depths}\")\n",
    "    print(f\"==> WINDOW_SIZE:        {window_size}\")\n",
    "    print(f\"==> MLP_RATIO:          {mlp_ratio}\")\n",
    "    print(f\"==> QKV_BIAS:           {qkv_bias}\")\n",
    "    print(f\"==> QK_SCALE:           {qk_scale}\")\n",
    "    print(f\"==> DROP_PATH_RATE:     {drop_path_rate}\")\n",
    "    print(f\"==> APE:                {ape}\")\n",
    "    print(f\"==> PATCH_NORM:         {patch_norm}\")\n",
    "    print(25 * \"=\" + \"FINISHED\" + 25 * \"=\")\n",
    "    model = SwinTransformer(image_size=image_size,\n",
    "                            patch_size=patch_size,\n",
    "                            in_chans=in_chans,\n",
    "                            num_classes=args.num_classes,\n",
    "                            embed_dim=embed_dim,\n",
    "                            depths=depths,\n",
    "                            num_heads=num_heads,\n",
    "                            window_size=window_size,\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            qk_scale=qk_scale,\n",
    "                            drop_rate=0.,\n",
    "                            drop_path_rate=drop_path_rate,\n",
    "                            ape=ape,\n",
    "                            patch_norm=patch_norm)\n",
    "    # print(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义EvaluateCallBack，保存在验证集上指标最优的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallBack(Callback):\n",
    "    \"\"\"EvaluateCallBack\"\"\"\n",
    "    \n",
    "    def __init__(self, model, eval_dataset):\n",
    "        super(EvaluateCallBack, self).__init__()\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.best_acc = 0.\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"\n",
    "            Test when epoch end, save best model with best.ckpt.\n",
    "        \"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch_num = cb_params.cur_epoch_num\n",
    "        result = self.model.eval(self.eval_dataset)\n",
    "        if result[\"Acc\"] > self.best_acc:\n",
    "            self.best_acc = result[\"Acc\"]\n",
    "            mindspore.save_checkpoint(cb_params.train_network, 'ckpt/best.ckpt')\n",
    "        print(\"epoch: %s acc: %s top5-acc: %s, best acc is %s\" %\n",
    "              (cb_params.cur_epoch_num, result[\"Acc\"], result[\"Top5-Acc\"], self.best_acc), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据、设置环境、初始化模型、训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args, training=True):\n",
    "    \"\"\"\"Get model according to args.set\"\"\"\n",
    "    print(f\"=> Getting {args.set} dataset\")\n",
    "    dataset = ImageNet(args, training)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def set_device(args):\n",
    "    \"\"\"Set device and ParallelMode(if device_num > 1)\"\"\"\n",
    "    rank = 0\n",
    "    # set context and device\n",
    "    device_target = args.device_target\n",
    "    device_num = int(os.environ.get(\"DEVICE_NUM\", 1))\n",
    "\n",
    "    if device_target == \"Ascend\":\n",
    "        if device_num > 1:\n",
    "            context.set_context(device_id=int(os.environ[\"DEVICE_ID\"]))\n",
    "            init(backend_name='hccl')\n",
    "            context.reset_auto_parallel_context()\n",
    "            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n",
    "                                              gradients_mean=True)\n",
    "            # context.set_auto_parallel_context(pipeline_stages=2, full_batch=True)\n",
    "\n",
    "            rank = get_rank()\n",
    "        else:\n",
    "            context.set_context(device_id=args.device_id)\n",
    "    elif device_target == \"GPU\":\n",
    "        if device_num > 1:\n",
    "            init(backend_name='nccl')\n",
    "            context.reset_auto_parallel_context()\n",
    "            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n",
    "                                              gradients_mean=True)\n",
    "            rank = get_rank()\n",
    "        else:\n",
    "            context.set_context(device_id=args.device_id)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported platform.\")\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================MODEL CONFIG=========================\n",
      "==> IMAGE_SIZE:         224\n",
      "==> PATCH_SIZE:         4\n",
      "==> NUM_CLASSES:        100\n",
      "==> EMBED_DIM:          96\n",
      "==> NUM_HEADS:          [3, 6, 12, 24]\n",
      "==> DEPTHS:             [2, 2, 6, 2]\n",
      "==> WINDOW_SIZE:        7\n",
      "==> MLP_RATIO:          4.0\n",
      "==> QKV_BIAS:           True\n",
      "==> QK_SCALE:           None\n",
      "==> DROP_PATH_RATE:     0.2\n",
      "==> APE:                False\n",
      "==> PATCH_NORM:         True\n",
      "=========================FINISHED=========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2932479:139625079657088,MainProcess):2022-10-22-17:23:09.484.580 [mindspore/train/model.py:1075] For EvaluateCallBack callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using amp_level O1\n",
      "=> change swin_tiny_patch4_window7_224 to fp16\n",
      "=> cast (<class 'mindspore.nn.layer.activation.GELU'>, <class 'mindspore.nn.layer.activation.Softmax'>, <class 'mindspore.nn.layer.conv.Conv2d'>, <class 'mindspore.nn.layer.conv.Conv1d'>, <class 'mindspore.nn.layer.normalization.BatchNorm2d'>, <class 'mindspore.nn.layer.normalization.LayerNorm'>) to fp32 back\n",
      "=========================Using MixBatch=========================\n",
      "=> Getting ImageNet dataset\n",
      "=> When using train_wrapper, using optimizer adamw\n",
      "=> Get LR from epoch: 0\n",
      "=> Start step: 0\n",
      "=> Total step: 32800\n",
      "=> Accumulation step:1\n",
      "=> Using DynamicLossScaleUpdateCell\n",
      "begin train\n",
      "=============Over Flow, skipping=============\n",
      "=============Over Flow, skipping=============\n",
      "=============Over Flow, skipping=============\n",
      "=============Over Flow, skipping=============\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 1 step: 328, loss is 4.606683254241943\n",
      "Train epoch time: 98225.719 ms, per step time: 299.469 ms\n",
      "epoch: 1 acc: 0.04364809782608696 top5-acc: 0.1559103260869565, best acc is 0.04364809782608696\n",
      "epoch: 2 step: 328, loss is 4.516416072845459\n",
      "Train epoch time: 77570.307 ms, per step time: 236.495 ms\n",
      "epoch: 2 acc: 0.05672554347826087 top5-acc: 0.1875, best acc is 0.05672554347826087\n",
      "epoch: 3 step: 328, loss is 4.447005748748779\n",
      "Train epoch time: 77713.935 ms, per step time: 236.933 ms\n",
      "epoch: 3 acc: 0.06725543478260869 top5-acc: 0.2027853260869565, best acc is 0.06725543478260869\n",
      "epoch: 4 step: 328, loss is 4.451318740844727\n",
      "Train epoch time: 77429.944 ms, per step time: 236.067 ms\n",
      "epoch: 4 acc: 0.07540760869565218 top5-acc: 0.2301290760869565, best acc is 0.07540760869565218\n",
      "epoch: 5 step: 328, loss is 4.380975246429443\n",
      "Train epoch time: 77789.145 ms, per step time: 237.162 ms\n",
      "epoch: 5 acc: 0.08967391304347826 top5-acc: 0.2686820652173913, best acc is 0.08967391304347826\n",
      "epoch: 6 step: 328, loss is 4.443111896514893\n",
      "Train epoch time: 78030.254 ms, per step time: 237.897 ms\n",
      "epoch: 6 acc: 0.11090353260869565 top5-acc: 0.3021399456521739, best acc is 0.11090353260869565\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 7 step: 328, loss is 4.38911247253418\n",
      "Train epoch time: 77914.136 ms, per step time: 237.543 ms\n",
      "epoch: 7 acc: 0.13671875 top5-acc: 0.33474864130434784, best acc is 0.13671875\n",
      "epoch: 8 step: 328, loss is 4.416530609130859\n",
      "Train epoch time: 78155.158 ms, per step time: 238.278 ms\n",
      "epoch: 8 acc: 0.12516983695652173 top5-acc: 0.335258152173913, best acc is 0.13671875\n",
      "epoch: 9 step: 328, loss is 4.346778392791748\n",
      "Train epoch time: 77911.331 ms, per step time: 237.535 ms\n",
      "epoch: 9 acc: 0.14673913043478262 top5-acc: 0.36905570652173914, best acc is 0.14673913043478262\n",
      "epoch: 10 step: 328, loss is 4.5370354652404785\n",
      "Train epoch time: 77869.734 ms, per step time: 237.408 ms\n",
      "epoch: 10 acc: 0.16559103260869565 top5-acc: 0.41389266304347827, best acc is 0.16559103260869565\n",
      "epoch: 11 step: 328, loss is 4.40866756439209\n",
      "Train epoch time: 77792.362 ms, per step time: 237.172 ms\n",
      "epoch: 11 acc: 0.16932744565217392 top5-acc: 0.42595108695652173, best acc is 0.16932744565217392\n",
      "epoch: 12 step: 328, loss is 4.258575439453125\n",
      "Train epoch time: 78045.822 ms, per step time: 237.945 ms\n",
      "epoch: 12 acc: 0.1883491847826087 top5-acc: 0.4429347826086957, best acc is 0.1883491847826087\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 13 step: 328, loss is 4.406404972076416\n",
      "Train epoch time: 78015.169 ms, per step time: 237.851 ms\n",
      "epoch: 13 acc: 0.19208559782608695 top5-acc: 0.45023777173913043, best acc is 0.19208559782608695\n",
      "epoch: 14 step: 328, loss is 4.288840293884277\n",
      "Train epoch time: 77976.810 ms, per step time: 237.734 ms\n",
      "epoch: 14 acc: 0.20074728260869565 top5-acc: 0.47452445652173914, best acc is 0.20074728260869565\n",
      "epoch: 15 step: 328, loss is 4.286988735198975\n",
      "Train epoch time: 77837.343 ms, per step time: 237.309 ms\n",
      "epoch: 15 acc: 0.20838994565217392 top5-acc: 0.491508152173913, best acc is 0.20838994565217392\n",
      "epoch: 16 step: 328, loss is 4.023094654083252\n",
      "Train epoch time: 78042.042 ms, per step time: 237.933 ms\n",
      "epoch: 16 acc: 0.21993885869565216 top5-acc: 0.5079823369565217, best acc is 0.21993885869565216\n",
      "epoch: 17 step: 328, loss is 4.169596195220947\n",
      "Train epoch time: 77697.700 ms, per step time: 236.883 ms\n",
      "epoch: 17 acc: 0.22554347826086957 top5-acc: 0.5061141304347826, best acc is 0.22554347826086957\n",
      "epoch: 18 step: 328, loss is 4.3070878982543945\n",
      "Train epoch time: 78022.776 ms, per step time: 237.874 ms\n",
      "epoch: 18 acc: 0.23539402173913043 top5-acc: 0.5283627717391305, best acc is 0.23539402173913043\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 19 step: 328, loss is 4.109980583190918\n",
      "Train epoch time: 78016.262 ms, per step time: 237.854 ms\n",
      "epoch: 19 acc: 0.25577445652173914 top5-acc: 0.5222486413043478, best acc is 0.25577445652173914\n",
      "epoch: 20 step: 328, loss is 4.031306743621826\n",
      "Train epoch time: 77715.961 ms, per step time: 236.939 ms\n",
      "epoch: 20 acc: 0.25900135869565216 top5-acc: 0.5385529891304348, best acc is 0.25900135869565216\n",
      "epoch: 21 step: 328, loss is 4.215546607971191\n",
      "Train epoch time: 77684.027 ms, per step time: 236.842 ms\n",
      "epoch: 21 acc: 0.25968070652173914 top5-acc: 0.5531589673913043, best acc is 0.25968070652173914\n",
      "epoch: 22 step: 328, loss is 4.093695640563965\n",
      "Train epoch time: 78090.945 ms, per step time: 238.082 ms\n",
      "epoch: 22 acc: 0.2600203804347826 top5-acc: 0.5543478260869565, best acc is 0.2600203804347826\n",
      "epoch: 23 step: 328, loss is 4.0735273361206055\n",
      "Train epoch time: 77968.515 ms, per step time: 237.709 ms\n",
      "epoch: 23 acc: 0.2787024456521739 top5-acc: 0.5686141304347826, best acc is 0.2787024456521739\n",
      "epoch: 24 step: 328, loss is 4.377508163452148\n",
      "Train epoch time: 77772.071 ms, per step time: 237.110 ms\n",
      "epoch: 24 acc: 0.2798913043478261 top5-acc: 0.5782948369565217, best acc is 0.2798913043478261\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 25 step: 328, loss is 3.937976360321045\n",
      "Train epoch time: 77997.789 ms, per step time: 237.798 ms\n",
      "epoch: 25 acc: 0.2916100543478261 top5-acc: 0.5825407608695652, best acc is 0.2916100543478261\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 26 step: 328, loss is 4.130610942840576\n",
      "Train epoch time: 77919.894 ms, per step time: 237.561 ms\n",
      "epoch: 26 acc: 0.29483695652173914 top5-acc: 0.5840692934782609, best acc is 0.29483695652173914\n",
      "epoch: 27 step: 328, loss is 3.963009834289551\n",
      "Train epoch time: 78296.125 ms, per step time: 238.708 ms\n",
      "epoch: 27 acc: 0.30570652173913043 top5-acc: 0.6103940217391305, best acc is 0.30570652173913043\n",
      "epoch: 28 step: 328, loss is 3.94366717338562\n",
      "Train epoch time: 78107.002 ms, per step time: 238.131 ms\n",
      "epoch: 28 acc: 0.31266983695652173 top5-acc: 0.6154891304347826, best acc is 0.31266983695652173\n",
      "epoch: 29 step: 328, loss is 3.7120132446289062\n",
      "Train epoch time: 77957.414 ms, per step time: 237.675 ms\n",
      "epoch: 29 acc: 0.32319972826086957 top5-acc: 0.627547554347826, best acc is 0.32319972826086957\n",
      "epoch: 30 step: 328, loss is 4.277674198150635\n",
      "Train epoch time: 78170.149 ms, per step time: 238.324 ms\n",
      "epoch: 30 acc: 0.311820652173913 top5-acc: 0.6129415760869565, best acc is 0.32319972826086957\n",
      "epoch: 31 step: 328, loss is 4.324661731719971\n",
      "Train epoch time: 78529.550 ms, per step time: 239.419 ms\n",
      "epoch: 31 acc: 0.3255774456521739 top5-acc: 0.6299252717391305, best acc is 0.3255774456521739\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 32 step: 328, loss is 4.174750804901123\n",
      "Train epoch time: 77924.515 ms, per step time: 237.575 ms\n",
      "epoch: 32 acc: 0.33695652173913043 top5-acc: 0.6441915760869565, best acc is 0.33695652173913043\n",
      "epoch: 33 step: 328, loss is 3.8301479816436768\n",
      "Train epoch time: 77852.323 ms, per step time: 237.355 ms\n",
      "epoch: 33 acc: 0.3422214673913043 top5-acc: 0.641983695652174, best acc is 0.3422214673913043\n",
      "epoch: 34 step: 328, loss is 4.2253923416137695\n",
      "Train epoch time: 78160.324 ms, per step time: 238.294 ms\n",
      "epoch: 34 acc: 0.32778532608695654 top5-acc: 0.6323029891304348, best acc is 0.3422214673913043\n",
      "epoch: 35 step: 328, loss is 4.316101551055908\n",
      "Train epoch time: 77949.068 ms, per step time: 237.650 ms\n",
      "epoch: 35 acc: 0.358695652173913 top5-acc: 0.6569293478260869, best acc is 0.358695652173913\n",
      "epoch: 36 step: 328, loss is 4.160644054412842\n",
      "Train epoch time: 78140.680 ms, per step time: 238.234 ms\n",
      "epoch: 36 acc: 0.32999320652173914 top5-acc: 0.6413043478260869, best acc is 0.358695652173913\n",
      "epoch: 37 step: 328, loss is 3.802969217300415\n",
      "Train epoch time: 78095.721 ms, per step time: 238.097 ms\n",
      "epoch: 37 acc: 0.36379076086956524 top5-acc: 0.6569293478260869, best acc is 0.36379076086956524\n",
      "epoch: 38 step: 328, loss is 4.089756011962891\n",
      "Train epoch time: 78088.570 ms, per step time: 238.075 ms\n",
      "epoch: 38 acc: 0.37313179347826086 top5-acc: 0.6798573369565217, best acc is 0.37313179347826086\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 39 step: 328, loss is 3.53753662109375\n",
      "Train epoch time: 77897.645 ms, per step time: 237.493 ms\n",
      "epoch: 39 acc: 0.3828125 top5-acc: 0.6759510869565217, best acc is 0.3828125\n",
      "epoch: 40 step: 328, loss is 3.609902858734131\n",
      "Train epoch time: 78314.654 ms, per step time: 238.764 ms\n",
      "epoch: 40 acc: 0.3804347826086957 top5-acc: 0.6703464673913043, best acc is 0.3828125\n",
      "epoch: 41 step: 328, loss is 4.303561210632324\n",
      "Train epoch time: 78110.555 ms, per step time: 238.142 ms\n",
      "epoch: 41 acc: 0.39809782608695654 top5-acc: 0.6931046195652174, best acc is 0.39809782608695654\n",
      "epoch: 42 step: 328, loss is 3.4438838958740234\n",
      "Train epoch time: 77831.506 ms, per step time: 237.291 ms\n",
      "epoch: 42 acc: 0.3931725543478261 top5-acc: 0.6961616847826086, best acc is 0.39809782608695654\n",
      "epoch: 43 step: 328, loss is 3.5862011909484863\n",
      "Train epoch time: 78138.125 ms, per step time: 238.226 ms\n",
      "epoch: 43 acc: 0.3967391304347826 top5-acc: 0.6983695652173914, best acc is 0.39809782608695654\n",
      "epoch: 44 step: 328, loss is 4.128260612487793\n",
      "Train epoch time: 77968.969 ms, per step time: 237.710 ms\n",
      "epoch: 44 acc: 0.39249320652173914 top5-acc: 0.6944633152173914, best acc is 0.39809782608695654\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 45 step: 328, loss is 4.155826091766357\n",
      "Train epoch time: 77874.741 ms, per step time: 237.423 ms\n",
      "epoch: 45 acc: 0.40591032608695654 top5-acc: 0.7055027173913043, best acc is 0.40591032608695654\n",
      "epoch: 46 step: 328, loss is 3.9320266246795654\n",
      "Train epoch time: 78020.188 ms, per step time: 237.866 ms\n",
      "epoch: 46 acc: 0.4154211956521739 top5-acc: 0.7155230978260869, best acc is 0.4154211956521739\n",
      "epoch: 47 step: 328, loss is 4.123641014099121\n",
      "Train epoch time: 78344.166 ms, per step time: 238.854 ms\n",
      "epoch: 47 acc: 0.4155910326086957 top5-acc: 0.7116168478260869, best acc is 0.4155910326086957\n",
      "epoch: 48 step: 328, loss is 4.011114120483398\n",
      "Train epoch time: 78071.050 ms, per step time: 238.021 ms\n",
      "epoch: 48 acc: 0.4232336956521739 top5-acc: 0.7280910326086957, best acc is 0.4232336956521739\n",
      "epoch: 49 step: 328, loss is 4.114437580108643\n",
      "Train epoch time: 78166.502 ms, per step time: 238.313 ms\n",
      "epoch: 49 acc: 0.4327445652173913 top5-acc: 0.7359035326086957, best acc is 0.4327445652173913\n",
      "epoch: 50 step: 328, loss is 4.075896739959717\n",
      "Train epoch time: 77756.753 ms, per step time: 237.063 ms\n",
      "epoch: 50 acc: 0.4201766304347826 top5-acc: 0.7141644021739131, best acc is 0.4327445652173913\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 51 step: 328, loss is 4.036683082580566\n",
      "Train epoch time: 77974.045 ms, per step time: 237.726 ms\n",
      "epoch: 51 acc: 0.43783967391304346 top5-acc: 0.7260529891304348, best acc is 0.43783967391304346\n",
      "epoch: 52 step: 328, loss is 4.108072280883789\n",
      "Train epoch time: 78001.574 ms, per step time: 237.810 ms\n",
      "epoch: 52 acc: 0.4320652173913043 top5-acc: 0.7331861413043478, best acc is 0.43783967391304346\n",
      "epoch: 53 step: 328, loss is 3.69376802444458\n",
      "Train epoch time: 77847.631 ms, per step time: 237.340 ms\n",
      "epoch: 53 acc: 0.4505774456521739 top5-acc: 0.7437160326086957, best acc is 0.4505774456521739\n",
      "epoch: 54 step: 328, loss is 4.165959358215332\n",
      "Train epoch time: 77814.628 ms, per step time: 237.240 ms\n",
      "epoch: 54 acc: 0.44089673913043476 top5-acc: 0.7328464673913043, best acc is 0.4505774456521739\n",
      "epoch: 55 step: 328, loss is 4.163305282592773\n",
      "Train epoch time: 78025.681 ms, per step time: 237.883 ms\n",
      "epoch: 55 acc: 0.45499320652173914 top5-acc: 0.7367527173913043, best acc is 0.45499320652173914\n",
      "epoch: 56 step: 328, loss is 4.002499580383301\n",
      "Train epoch time: 78114.233 ms, per step time: 238.153 ms\n",
      "epoch: 56 acc: 0.45822010869565216 top5-acc: 0.7454144021739131, best acc is 0.45822010869565216\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 57 step: 328, loss is 3.964570999145508\n",
      "Train epoch time: 77909.596 ms, per step time: 237.529 ms\n",
      "epoch: 57 acc: 0.47673233695652173 top5-acc: 0.7567934782608695, best acc is 0.47673233695652173\n",
      "epoch: 58 step: 328, loss is 3.8768999576568604\n",
      "Train epoch time: 77917.612 ms, per step time: 237.554 ms\n",
      "epoch: 58 acc: 0.47843070652173914 top5-acc: 0.7554347826086957, best acc is 0.47843070652173914\n",
      "epoch: 59 step: 328, loss is 3.3266918659210205\n",
      "Train epoch time: 77987.581 ms, per step time: 237.767 ms\n",
      "epoch: 59 acc: 0.4782608695652174 top5-acc: 0.7637567934782609, best acc is 0.47843070652173914\n",
      "epoch: 60 step: 328, loss is 3.9986932277679443\n",
      "Train epoch time: 78162.117 ms, per step time: 238.299 ms\n",
      "epoch: 60 acc: 0.4748641304347826 top5-acc: 0.7651154891304348, best acc is 0.47843070652173914\n",
      "epoch: 61 step: 328, loss is 3.554734230041504\n",
      "Train epoch time: 79327.994 ms, per step time: 241.854 ms\n",
      "epoch: 61 acc: 0.47690217391304346 top5-acc: 0.7627377717391305, best acc is 0.47843070652173914\n",
      "epoch: 62 step: 328, loss is 4.098214149475098\n",
      "Train epoch time: 79343.368 ms, per step time: 241.901 ms\n",
      "epoch: 62 acc: 0.48012907608695654 top5-acc: 0.765625, best acc is 0.48012907608695654\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 63 step: 328, loss is 3.7967209815979004\n",
      "Train epoch time: 78342.405 ms, per step time: 238.849 ms\n",
      "epoch: 63 acc: 0.48556385869565216 top5-acc: 0.7676630434782609, best acc is 0.48556385869565216\n",
      "epoch: 64 step: 328, loss is 3.398728609085083\n",
      "Train epoch time: 78146.445 ms, per step time: 238.251 ms\n",
      "epoch: 64 acc: 0.4835258152173913 top5-acc: 0.7725883152173914, best acc is 0.48556385869565216\n",
      "epoch: 65 step: 328, loss is 3.307548999786377\n",
      "Train epoch time: 77922.219 ms, per step time: 237.568 ms\n",
      "epoch: 65 acc: 0.4984714673913043 top5-acc: 0.778702445652174, best acc is 0.4984714673913043\n",
      "epoch: 66 step: 328, loss is 4.043028354644775\n",
      "Train epoch time: 78039.694 ms, per step time: 237.926 ms\n",
      "epoch: 66 acc: 0.5010190217391305 top5-acc: 0.7793817934782609, best acc is 0.5010190217391305\n",
      "epoch: 67 step: 328, loss is 3.7040460109710693\n",
      "Train epoch time: 77856.072 ms, per step time: 237.366 ms\n",
      "epoch: 67 acc: 0.49966032608695654 top5-acc: 0.7797214673913043, best acc is 0.5010190217391305\n",
      "epoch: 68 step: 328, loss is 3.765737771987915\n",
      "Train epoch time: 78541.913 ms, per step time: 239.457 ms\n",
      "epoch: 68 acc: 0.5047554347826086 top5-acc: 0.783797554347826, best acc is 0.5047554347826086\n",
      "epoch: 69 step: 328, loss is 3.8921711444854736\n",
      "Train epoch time: 78037.338 ms, per step time: 237.919 ms\n",
      "epoch: 69 acc: 0.5137567934782609 top5-acc: 0.7843070652173914, best acc is 0.5137567934782609\n",
      "epoch: 70 step: 328, loss is 3.996326446533203\n",
      "Train epoch time: 77872.417 ms, per step time: 237.416 ms\n",
      "epoch: 70 acc: 0.5125679347826086 top5-acc: 0.7875339673913043, best acc is 0.5137567934782609\n",
      "epoch: 71 step: 328, loss is 4.144741535186768\n",
      "Train epoch time: 77957.219 ms, per step time: 237.674 ms\n",
      "epoch: 71 acc: 0.5137567934782609 top5-acc: 0.790421195652174, best acc is 0.5137567934782609\n",
      "epoch: 72 step: 328, loss is 3.6382193565368652\n",
      "Train epoch time: 78040.411 ms, per step time: 237.928 ms\n",
      "epoch: 72 acc: 0.5127377717391305 top5-acc: 0.7860054347826086, best acc is 0.5137567934782609\n",
      "epoch: 73 step: 328, loss is 3.5755395889282227\n",
      "Train epoch time: 78034.322 ms, per step time: 237.910 ms\n",
      "epoch: 73 acc: 0.5159646739130435 top5-acc: 0.7878736413043478, best acc is 0.5159646739130435\n",
      "epoch: 74 step: 328, loss is 3.6679434776306152\n",
      "Train epoch time: 77881.833 ms, per step time: 237.445 ms\n",
      "epoch: 74 acc: 0.5193614130434783 top5-acc: 0.7931385869565217, best acc is 0.5193614130434783\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 75 step: 328, loss is 3.8212318420410156\n",
      "Train epoch time: 78212.435 ms, per step time: 238.453 ms\n",
      "epoch: 75 acc: 0.5236073369565217 top5-acc: 0.7936480978260869, best acc is 0.5236073369565217\n",
      "epoch: 76 step: 328, loss is 3.811330556869507\n",
      "Train epoch time: 77924.677 ms, per step time: 237.575 ms\n",
      "epoch: 76 acc: 0.5263247282608695 top5-acc: 0.7961956521739131, best acc is 0.5263247282608695\n",
      "epoch: 77 step: 328, loss is 3.0529000759124756\n",
      "Train epoch time: 77880.984 ms, per step time: 237.442 ms\n",
      "epoch: 77 acc: 0.5264945652173914 top5-acc: 0.7990828804347826, best acc is 0.5264945652173914\n",
      "epoch: 78 step: 328, loss is 3.051379680633545\n",
      "Train epoch time: 77631.681 ms, per step time: 236.682 ms\n",
      "epoch: 78 acc: 0.5320991847826086 top5-acc: 0.8023097826086957, best acc is 0.5320991847826086\n",
      "epoch: 79 step: 328, loss is 4.120721340179443\n",
      "Train epoch time: 77824.862 ms, per step time: 237.271 ms\n",
      "epoch: 79 acc: 0.53125 top5-acc: 0.7989130434782609, best acc is 0.5320991847826086\n",
      "epoch: 80 step: 328, loss is 3.954958915710449\n",
      "Train epoch time: 77628.485 ms, per step time: 236.672 ms\n",
      "epoch: 80 acc: 0.5353260869565217 top5-acc: 0.8002717391304348, best acc is 0.5353260869565217\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 81 step: 328, loss is 3.724217176437378\n",
      "Train epoch time: 78065.812 ms, per step time: 238.006 ms\n",
      "epoch: 81 acc: 0.5363451086956522 top5-acc: 0.8019701086956522, best acc is 0.5363451086956522\n",
      "epoch: 82 step: 328, loss is 3.155369997024536\n",
      "Train epoch time: 77924.482 ms, per step time: 237.575 ms\n",
      "epoch: 82 acc: 0.537703804347826 top5-acc: 0.8067255434782609, best acc is 0.537703804347826\n",
      "epoch: 83 step: 328, loss is 3.3825459480285645\n",
      "Train epoch time: 77953.530 ms, per step time: 237.663 ms\n",
      "epoch: 83 acc: 0.5354959239130435 top5-acc: 0.803328804347826, best acc is 0.537703804347826\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 84 step: 328, loss is 3.9255869388580322\n",
      "Train epoch time: 77817.596 ms, per step time: 237.249 ms\n",
      "epoch: 84 acc: 0.5433084239130435 top5-acc: 0.8092730978260869, best acc is 0.5433084239130435\n",
      "epoch: 85 step: 328, loss is 3.62528133392334\n",
      "Train epoch time: 78312.543 ms, per step time: 238.758 ms\n",
      "epoch: 85 acc: 0.5436480978260869 top5-acc: 0.8045176630434783, best acc is 0.5436480978260869\n",
      "epoch: 86 step: 328, loss is 3.8779077529907227\n",
      "Train epoch time: 79035.696 ms, per step time: 240.962 ms\n",
      "epoch: 86 acc: 0.5456861413043478 top5-acc: 0.80859375, best acc is 0.5456861413043478\n",
      "epoch: 87 step: 328, loss is 4.07357931137085\n",
      "Train epoch time: 77741.437 ms, per step time: 237.017 ms\n",
      "epoch: 87 acc: 0.544327445652174 top5-acc: 0.8034986413043478, best acc is 0.5456861413043478\n",
      "epoch: 88 step: 328, loss is 3.640528917312622\n",
      "Train epoch time: 77927.202 ms, per step time: 237.583 ms\n",
      "epoch: 88 acc: 0.5438179347826086 top5-acc: 0.8074048913043478, best acc is 0.5456861413043478\n",
      "epoch: 89 step: 328, loss is 3.3842782974243164\n",
      "Train epoch time: 78080.250 ms, per step time: 238.050 ms\n",
      "epoch: 89 acc: 0.5448369565217391 top5-acc: 0.8108016304347826, best acc is 0.5456861413043478\n",
      "epoch: 90 step: 328, loss is 3.9701128005981445\n",
      "Train epoch time: 78034.974 ms, per step time: 237.912 ms\n",
      "epoch: 90 acc: 0.5473845108695652 top5-acc: 0.8106317934782609, best acc is 0.5473845108695652\n",
      "epoch: 91 step: 328, loss is 3.93862247467041\n",
      "Train epoch time: 77777.080 ms, per step time: 237.125 ms\n",
      "epoch: 91 acc: 0.5487432065217391 top5-acc: 0.8091032608695652, best acc is 0.5487432065217391\n",
      "epoch: 92 step: 328, loss is 3.3578507900238037\n",
      "Train epoch time: 77840.785 ms, per step time: 237.319 ms\n",
      "epoch: 92 acc: 0.5499320652173914 top5-acc: 0.8087635869565217, best acc is 0.5499320652173914\n",
      "epoch: 93 step: 328, loss is 3.402940034866333\n",
      "Train epoch time: 77841.572 ms, per step time: 237.322 ms\n",
      "epoch: 93 acc: 0.5516304347826086 top5-acc: 0.8077445652173914, best acc is 0.5516304347826086\n",
      "epoch: 94 step: 328, loss is 3.50596022605896\n",
      "Train epoch time: 78055.788 ms, per step time: 237.975 ms\n",
      "epoch: 94 acc: 0.5514605978260869 top5-acc: 0.8106317934782609, best acc is 0.5516304347826086\n",
      "epoch: 95 step: 328, loss is 3.9884400367736816\n",
      "Train epoch time: 77863.441 ms, per step time: 237.389 ms\n",
      "epoch: 95 acc: 0.5516304347826086 top5-acc: 0.8092730978260869, best acc is 0.5516304347826086\n",
      "=============Over Flow, skipping=============\n",
      "epoch: 96 step: 328, loss is 3.4499902725219727\n",
      "Train epoch time: 77863.299 ms, per step time: 237.388 ms\n",
      "epoch: 96 acc: 0.5526494565217391 top5-acc: 0.8079144021739131, best acc is 0.5526494565217391\n",
      "epoch: 97 step: 328, loss is 3.518087387084961\n",
      "Train epoch time: 77672.538 ms, per step time: 236.807 ms\n",
      "epoch: 97 acc: 0.5526494565217391 top5-acc: 0.8102921195652174, best acc is 0.5526494565217391\n",
      "epoch: 98 step: 328, loss is 2.900960922241211\n",
      "Train epoch time: 77649.499 ms, per step time: 236.736 ms\n",
      "epoch: 98 acc: 0.5512907608695652 top5-acc: 0.8084239130434783, best acc is 0.5526494565217391\n",
      "epoch: 99 step: 328, loss is 3.933858633041382\n",
      "Train epoch time: 77925.955 ms, per step time: 237.579 ms\n",
      "epoch: 99 acc: 0.5531589673913043 top5-acc: 0.8104619565217391, best acc is 0.5531589673913043\n",
      "epoch: 100 step: 328, loss is 2.9689321517944336\n",
      "Train epoch time: 77380.916 ms, per step time: 235.917 ms\n",
      "epoch: 100 acc: 0.5514605978260869 top5-acc: 0.809952445652174, best acc is 0.5531589673913043\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "assert args.crop, f\"{args.arch} is only for evaluation\"\n",
    "set_seed(args.seed)\n",
    "mode = {\n",
    "    0: context.GRAPH_MODE,\n",
    "    1: context.PYNATIVE_MODE\n",
    "}\n",
    "context.set_context(mode=mode[args.graph_mode], device_target=args.device_target)\n",
    "context.set_context(enable_graph_kernel=True)\n",
    "if args.device_target == \"Ascend\":\n",
    "    context.set_context(enable_auto_mixed_precision=True)\n",
    "rank = set_device(args)\n",
    "\n",
    "# get model and cast amp_level\n",
    "net = get_swintransformer(args)\n",
    "cast_amp(net)\n",
    "criterion = get_criterion(args)\n",
    "net_with_loss = NetWithLoss(net, criterion)\n",
    "\n",
    "data = get_dataset(args)\n",
    "batch_num = data.train_dataset.get_dataset_size()\n",
    "optimizer = get_optimizer(args, net, batch_num)\n",
    "\n",
    "net_with_loss = get_train_one_step(args, net_with_loss, optimizer)\n",
    "\n",
    "eval_network = nn.WithEvalCell(net, criterion, args.amp_level in [\"O2\", \"O3\", \"auto\"])\n",
    "eval_indexes = [0, 1, 2]\n",
    "\n",
    "eval_metrics = {'Loss': nn.Loss(),\n",
    "                'Acc': nn.Accuracy(),\n",
    "                'Top5-Acc': nn.Top5CategoricalAccuracy()}\n",
    "\n",
    "model = Model(net_with_loss, metrics=eval_metrics,\n",
    "              eval_network=eval_network,\n",
    "              eval_indexes=eval_indexes)\n",
    "\n",
    "# checkpoint and callback settings\n",
    "# keep_checkpoint_max (int) - 最多保存多少个checkpoint文件。默认值：1。\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=data.train_dataset.get_dataset_size(),\n",
    "                             keep_checkpoint_max=args.save_every)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "\n",
    "time_cb = TimeMonitor(data_size=data.train_dataset.get_dataset_size())\n",
    "# prefix (str) - checkpoint文件的前缀名称。默认值：’CKP’。\n",
    "# directory (str) - 保存checkpoint文件的文件夹路径。默认情况下，文件保存在当前目录下。默认值：None。\n",
    "# config (CheckpointConfig) - checkpoint策略配置。默认值：None。\n",
    "ckpoint_cb = ModelCheckpoint(prefix=args.arch, directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()\n",
    "eval_cb = EvaluateCallBack(model=model, eval_dataset=data.val_dataset)\n",
    "\n",
    "print(\"begin train\")\n",
    "model.train(int(args.epochs - args.start_epoch), data.train_dataset,\n",
    "            callbacks=[time_cb, ckpoint_cb, loss_cb, eval_cb],\n",
    "            dataset_sink_mode=True)\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, model):\n",
    "    \n",
    "    # load the saved model for evaluation\n",
    "    param_dict = load_checkpoint(\"ckpt/best.ckpt\")\n",
    "    # load parameter to the network\n",
    "    load_param_into_net(net, param_dict)\n",
    "    # load testing dataset\n",
    "    data = get_dataset(args, training=False)\n",
    "    \n",
    "    print(f\"=> begin eval\")\n",
    "    results = model.eval(data.test_dataset)\n",
    "    print(f\"=> eval results:{results}\")\n",
    "    print(f\"=> eval success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Getting ImageNet dataset\n",
      "=> begin eval\n",
      "=> eval results:{'Loss': 1.8583465449271663, 'Acc': 0.5525873655913979, 'Top5-Acc': 0.8158602150537635}\n",
      "=> eval success\n"
     ]
    }
   ],
   "source": [
    "test(net, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "miniImageNet上的SwinTransformer\n",
    "\n",
    "| 属性 | 情况  |\n",
    "| --- | --- |\n",
    "| 模型 | SwinTransformer|\n",
    "| 模型版本 | swin_tiny_patch4_window7_224 |\n",
    "| 资源 | Gefore RTX 3090 * 1 |\n",
    "| MindSpore版本 | 1.8.1 |\n",
    "| 验证集 | miniImageNet Val，共6,000张图像 |\n",
    "| 验证集分类准确率 | Top1-Acc: 55.32%, Top5-Acc: 81.05% |\n",
    "| 测试集 | miniImageNet Test，共12,000张图像 |\n",
    "| 测试集分类准确率 | Top1-Acc: 55.26%, Top5-Acc: 81.59% |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4df7a6600e22bae99e6e8f837be5af686fd7a404512ca9c2620376f38fe7d31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
